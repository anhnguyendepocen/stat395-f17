---
title: "Class 22: Vector Representations of Words"
author: "Taylor Arnold"
output: html_notebook
---




{% highlight r %}
library(readr)
library(dplyr)
library(ggplot2)
library(stringi)
library(tokenizers)
library(smodels)
library(keras)
{% endhighlight %}

## Embeddings

Our initial approach to working with textual data involved counting
the occurance of words, characters, or sequences of either. These
were summarized in a term frequence matrix, which we typically then
fed into an elastic net given the high-dimensionality of the dataset.
What subtleties are lost here?

- word placement in the text
- word forms
- negation
- context
- semantic meaning

We briefly tried to address some of these by using a text parser, but
these these amounted to simply creating more subtle versions of counting
the occurance of things. We have not yet seen a realy way of working
with the true order of the text.

How can we use a neural network to solve this problem? The output structure
is easy, and matches the one-hot encoding of the image processing tasks.
(from the idea that just one of the bytes is `hot', i.e. turned on.)
What about the input layer though? How do we feed a block of text into a
neural network?

Let's first simplify the problem and think about just a single word at a
time. How can we represent even a single word as an input to a neural
network? One approach is to determine a *vocabulary* of terms, these
are all of the words that we want to support in the classification task.
Usually we construct this by looking at all of the snippets of text and
taking the N-most commonly occurring words. Any words in the texts not
in this vocabulary are removed before further training. This is the same
thing we did with term frequency matricies, but now we are considering
just a single word.

Once we have this vocabulary, we can represent a single word by
an $N$-length binary vector with exactly one $1$:
$$ \text{apple} \rightarrow [0,0,0,\ldots,0,1,0,\ldots,0] $$
This is just another one-hot representation.

Suppose we use a one-hot representation as the first layer in a neural
network. If this is followed directly by a dense layer with p hidden
neurons, the weights in the layer can be defined as an N-by-p matrix
W. In this special case we do not need a bias term, because we already
know the scale of the previous layer (0's and 1's).

For a given set of weights W, because of the one-hot representation, the
values of the outputs from the first hidden layer will simply be row j
of the matrix W, where j is the index of the input word in the vocabulary.

![](../assets/img/tikz40.png)

A word embedding is nothing more than a compression of a one-hot
representation and a dense hidden layer in a neural network. There
is not need to actually create the one-hot vector, and multiply by
all of W. We can just go directly from the index of the word in
the vocabulary, and read off of the j-th row of W.

What are we doing here, really? The whole idea is to map a word as
a vector in a p-dimensional space:

$$ f(\text{word}) \rightarrow \mathbb{R}^p $$

This is very similar to the transfer learning we did with images,
where each image was projected into a 512-dimensional space.

This is great, but most of the time we want to work with
a collection of words (a document) all as one entity. A
simple way to do this is to apply the word embedding to
each term, and the collapse (flatten) these into a single
long vector.

So if we have T terms in a document and a word embedding
with p terms, the output from the embedding layer will be
of size T times p. To be clear, the embedding step is agnostic
to the position of the word, much like the shared weights in a
convolutional neural network. The word "apple" is matched to
the same vector regardless of where in the sentence it is
found.

We will persist in one simplification today: assuming that
each document has the same number of terms. This is done by
truncating at some small number of words (less than what
most of the movie reviews are) and filling in any trailing
space by a special embedding of zeros. Though as you will see,
this is easy to rectify for working with text of larger sizes.

## Predicting letter bigrams

As a starting example, let's look again at the Amazon dataset
predicting which category a review comes from. This time, however,
we will not be predicting the class of the item. We are just
using the text data as a good source of natural English language
usage.


{% highlight r %}
amazon <- read_csv("https://statsmaths.github.io/ml_data/amazon_product_class.csv")
amazon <- amazon[-c(3056, 7876),]
{% endhighlight %}

Instead of embedding entire words, we will initially consider
embedding character bigrams:

$$ f(\text{character bigram}) \rightarrow \mathbb{R}^p $$

Our prediction task will be to look at a window of 9 bigrams
(18 letters) within the text, using the first 4 bigrams and
last 4 bigrams to predict the middle two bigrams. For example,
if we consider the phrase:

> I was quite sleepy

We take the lower case version (removing punctionation marks)
and chop it up into bigrams:

> [I ][wa][s ][qu][it][e ][sl][ee][py]

The goal is to see the following:

> [I ][wa][s ][qu][??][e ][sl][ee][py]

And use the the context to predict that the missing piece is
the bigram "it".

As a first step we need to construc the dataset. We remove all
the non-letters and spaces, convert to lower case, and tokenize
by characters. We then paste together pairs from the first 20
letters to get the required bigrams (we only need the first 9
bigrams, but I grabbed more to illustrate the point).


{% highlight r %}
txt <- stri_replace_all(amazon$text, "", regex = "[^a-zA-Z ]")
txt <- stri_trans_tolower(txt)
chars <- tokenize_characters(txt, strip_non_alphanum = FALSE)
chars <- Filter(function(v) length(v) > 200, chars)
chars <- lapply(chars, function(v) {
  apply(matrix(v[1:20], ncol = 2, byrow = TRUE), 1, paste, collapse = "")
  })
head(chars, n = 3)
{% endhighlight %}



{% highlight text %}
## [[1]]
##  [1] "at" " s" "om" "e " "po" "in" "t " "i " "wo" "ul"
##
## [[2]]
##  [1] "jo" "hn" " c" "ro" "wl" "ey" " w" "ro" "te" "  "
##
## [[3]]
##  [1] "th" "e " "ne" "w " "yo" "rk" " p" "os" "t " "is"
{% endhighlight %}

Next we create a vector `char_vals` listing all of the possible
bigrams occuring in the dataset, and convert the bigrams into
integer codes mapping into these values.


{% highlight r %}
char_vals <- unique(unlist(chars))
id <- lapply(chars, function(v) match(v, char_vals))
id <- matrix(unlist(id), ncol = 10, byrow = TRUE)
head(id)
{% endhighlight %}



{% highlight text %}
##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    1    2    3    4    5    6    7    8    9    10
## [2,]   11   12   13   14   15   16   17   14   18    19
## [3,]   20    4   21   22   23   24   25   26    7    27
## [4,]    6   28   29   30   31   32   33   34   35    36
## [5,]   37   38   29   39   17    6   36   40   41    42
## [6,]   43   44   45   46   47   48   49   27   28    50
{% endhighlight %}

Now, the data matrix `X` consists of columns 1 through 4
and 6 through 9. The response `y` uses the 5th column, offset
by 1 to make the category ids match the zero-index used by
**keras**. We don't have a training and testing set here, so
I will construct one manually.


{% highlight r %}
X <- id[,c(1:4,6:9)]
y <- id[,5] - 1
train_id <- rep(c("train", "test"), length.out = length(y))
{% endhighlight %}

Now, process the training data for the neural network.


{% highlight r %}
y_train <- to_categorical(y[train_id == "train"],
                          num_classes = length(char_vals))
X_train <- X[train_id == "train",]
{% endhighlight %}

We proceed by constructing a `layer_embedding` in keras. We
supply the length of the vocabulary (`input_dim`), the size
of the embedding (`output_dim`) and the the number of terms
in the input (the columns of `X`, `input_length`). The output
will not be a matrix, so we flatten the embedding, apply a dense
layer, and the softmax the expected probabilities.


{% highlight r %}
model <- keras_model_sequential()
model %>%
  layer_embedding(
    input_dim = length(char_vals),
    output_dim = 50,
    input_length = ncol(X)
    ) %>%
  layer_flatten() %>%
  layer_dense(256) %>%
  layer_activation("relu")%>%
  layer_dense(length(char_vals)) %>%
  layer_activation("softmax")
model
{% endhighlight %}



{% highlight text %}
## Model
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #
## ===========================================================================
## embedding_1 (Embedding)          (None, 8, 50)                 25500
## ___________________________________________________________________________
## flatten_1 (Flatten)              (None, 400)                   0
## ___________________________________________________________________________
## dense_1 (Dense)                  (None, 256)                   102656
## ___________________________________________________________________________
## activation_1 (Activation)        (None, 256)                   0
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 510)                   131070
## ___________________________________________________________________________
## activation_2 (Activation)        (None, 510)                   0
## ===========================================================================
## Total params: 259,226
## Trainable params: 259,226
## Non-trainable params: 0
## ___________________________________________________________________________
{% endhighlight %}

Can you figure out where the number of parameters in the
embedding layer comes from?

From here, the compiling and training the model uses the
exact some code as with dense and convolutional neural networks.


{% highlight r %}
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = "accuracy"
)
history <- model %>% fit(X_train, y_train, batch_size = 32,
              epochs = 10,
              validation_split = 0.1)
y_pred <- predict_classes(model, X) + 1
plot(history)
{% endhighlight %}

<img src="../assets/2017-11-14-class22/unnamed-chunk-8-1.png" title="plot of chunk unnamed-chunk-8" alt="plot of chunk unnamed-chunk-8" width="100%" />

The validation error rate is around 40%, not bad considering that
this is classification with a categorical variable having 100s of
values.

We can put the predictions together to see what the neural network
predicts for some example bigrams:


{% highlight r %}
prefix <- apply(matrix(char_vals[X[,1:4]], ncol = 4, byrow=FALSE),
                1, paste, collapse = "")
suffix <- apply(matrix(char_vals[X[,5:8]], ncol = 4, byrow=FALSE),
                1, paste, collapse = "")
y_char <- char_vals[y]
strs <- sprintf("%s[%s]%s", prefix, char_vals[y_pred],
                 suffix)
sample(strs[train_id == "test"], 100)
{% endhighlight %}



{% highlight text %}
##   [1] "cinderel[an] ii lady" "i have b[ee]n orderi" "when i f[ir]st saw t"
##   [4] "this boo[k ]was a po" "i have l[ik]ed this " "wow was [to] happy t"
##   [7] "this mov[ie] stays t" " calorie[ r]each  br" "this is [my] favorit"
##  [10] "so the a[ h]hor has " "the firs[t ]time i v" "my daugh[te]r wanted"
##  [13] "didnt re[al]ly care " "shortly [g ]fore suf" "like all[ o]f the th"
##  [16] "joseph s[s ]fanos pr" "ive neve[r ]been a f" "my daugh[te]r recomm"
##  [19] "soso idi[ed] teen co" "our offi[he] drinks " "this isn[si]like the"
##  [22] "best of [re]mes is a" "i wrote [e ]review f" "this isn[a ]a review"
##  [25] "i purcha[se]d this a" "it was a[re]aper bag" "ive been[ u]sing thi"
##  [28] "i was lo[ok]ing for " "i must s[ay] that wh" "ive have[r ]ong love"
##  [31] "sandra i[t ]very muc" "i love t[hi]s little" "we use t[hi]s stuff "
##  [34] "this mov[ie] impress" "this mov[ie] is a tr" "kat has [an] indepen"
##  [37] "after se[ s]ching an" "i watche[d ]this sto" "this ids[an] great t"
##  [40] "it seems[ t]any cust" "as of th[is] writing" "until no[t ]my perce"
##  [43] "i saw th[is] film wh" "i loved [th]is book " "i saw th[is] when li"
##  [46] "richard [co]gner is " "for as l[ea]g as ive" "i bought[ t]his prod"
##  [49] "very goo[d ]plot ver" "the only[le]rawback " "well i w[as]ted to t"
##  [52] "mac and [to]eese are" "martins [a ]test ins" "though t[hi] conceit"
##  [55] "it behav[is] as expe" "my son r[ea]d this i" "this boo[k ]is based"
##  [58] "in case [to]u were w" "yacun sy[en]p is an " "for thos[e ]who have"
##  [61] "definite[ly] tastes " "i ordere[d ]nettlele" "this is [an]irly exp"
##  [64] "flavor o[re]nature v" "i loved [th]is book " "there is[ a]ne heck "
##  [67] "bing and[ t]obs seco" "this is [an] importa" "patrick [a ]ldoon st"
##  [70] "clearly [th]is movie" "i almost[ d]idnt by " "this vid[ie] is so g"
##  [73] "please d[o ]cribe th" "i rememb[er] when ma" "the lege[an] of bogg"
##  [76] "as a pro[e ]i someti" "i love c[ed] cherry " "ok so th[er] is no f"
##  [79] "there is[ t]o questi" "ive neve[ t]heard of" "even aft[ a] repeat "
##  [82] "yes it s[ea]lls like" "baxter h[an] several" "i have t[he] tiny to"
##  [85] "my favor[th]e joan b" "first i [to]ught thi" "if you l[ik]e world "
##  [88] "the furt[in]r my son" "i origin[al]ly found" "british [re]ngster f"
##  [91] "other re[ad]ers have" "this is [a ]ur typic" "i was ne[al]to desig"
##  [94] "the mons[le]r trap i" "andy cap[an]fries ha" "this is [a ]great dv"
##  [97] "this rev[ie]w is for" "in novem[in]r  forme" "this is [a ]star fil"
## [100] "you must[ t]uy this "
{% endhighlight %}

Perhaps more impressive than the correct response are the type of
human-like errors it makes.

The embedding layer consists of a 510-by-50 matrix. This gives the
projection of each bigram in 50-dimensional space. We can access
this projection with the function `get_layer`.


{% highlight r %}
embed <- model$get_layer(index = 1L)$get_weights()[[1]]
dim(embed)
{% endhighlight %}



{% highlight text %}
## [1] 510  50
{% endhighlight %}

Visualizing the embedding is hard because it exists in 50-dimensional
space. We can take the embedding and apply t-SNE to do dimensionality
reduction:


{% highlight r %}
library(Rtsne)
tsne <- Rtsne(embed, dims = 2)
df <- data_frame(tsne1 = tsne$Y[,1],
                 tsne2 = tsne$Y[,2],
                 bgram = char_vals)
df$bgram <- stri_replace_all(df$bgram, "-", fixed = " ")
ggplot(df, aes(tsne1, tsne2)) +
  geom_text(aes(label = bgram)) +
  theme_minimal()
{% endhighlight %}

<img src="../assets/2017-11-14-class22/unnamed-chunk-11-1.png" title="plot of chunk unnamed-chunk-11" alt="plot of chunk unnamed-chunk-11" width="100%" />

I did not see any particularly noticable patterns, but we will see
that the word variant of this often shows off understandable patterns
in the data.

## Word embeddings

Now, let's use actual work embeddings to classify Amazon products.
As with the bigram embeddings, we first need to figure out the vocabulary
of terms we will allow in the model. Unlike with bigrams, there are too
many unique words to include them all. Instead, I will use just the most
frequently used 5000 words (it will actually be slightly more than this
because `top_n`, when faced with ties, will create a slightly larger
set rather than randomly selecting just 5000).


{% highlight r %}
words <- tokenize_words(amazon$text)
vocab <- top_n(count(data_frame(word = unlist(words)), word), n = 5000)$word
{% endhighlight %}



{% highlight text %}
## Selecting by n
{% endhighlight %}



{% highlight r %}
head(vocab, 50)
{% endhighlight %}



{% highlight text %}
##  [1] "0"     "02"    "1"     "10"    "100"   "11"    "12"    "13"
##  [9] "13th"  "14"    "15"    "16"    "17"    "18"    "19"    "1930"
## [17] "1932"  "1933"  "1934"  "1936"  "1939"  "1940"  "1942"  "1945"
## [25] "1947"  "1948"  "1950"  "1953"  "1960"  "1960s" "1968"  "1970"
## [33] "1970s" "1971"  "1972"  "1973"  "1974"  "1975"  "1980"  "1980s"
## [41] "1989"  "1990"  "1995"  "1996"  "1998"  "19th"  "1st"   "2"
## [49] "20"    "200"
{% endhighlight %}

With this vocabulary, we next create numeric indicies, throwing
away terms not in our vocabulary.


{% highlight r %}
id <- lapply(words, function(v) match(v, vocab))
id <- lapply(id, function(v) v[!is.na(v)])
id[1:3]
{% endhighlight %}



{% highlight text %}
## [[1]]
##  [1]  381 4190 3430 2299 5067 2696 4642 4732 3699 2931 4988 2046   62 3229
## [15] 3261 3258 3165   74 2299 1487 2754 3165  653 3818 3611 2299 3590 2076
## [29] 4035 3983   70 3229 1040 3258 3165   48 3189 2456   10 3229  719 3258
## [43] 3165    7 2445 2982 4161  680 2299 2696 2456 3125 2150 2039 4642  100
## [57] 3430 4978 2299 2149 2456 1873  622
##
## [[2]]
##   [1] 2499 5083 2723  533 2346 4559 5049 3165 2914  588 1502 3165 4559 4926
##  [15] 3462 4815  274  476 2456 2445 4559 2062 1730 3123 4174 1731 5081  684
##  [29]  270  261  274 2314 2891 4031 2154 4964 3189 4642 3932 2456 4925 3344
##  [43] 4559  521 3123 5081  684  270  261  243 5083 4559  100 1901 4892 4025
##  [57] 5025 4606  274  100 2104 4893 3165 2060 1901 4098  270 2355 3165 2220
##  [71] 4480 4573  336 2720 2934 3165  680 4846 4846 1774 3600 1730 5077  190
##  [85] 4614  108 1901 2287 3269 3165 4593 2299 1899 4573 2445 3678 3117 2192
##  [99] 4556 2445 3111 1108 2346 4559 3285 2715  109 1285 4559 3420  100  274
## [113] 2220 4831 5054 3189  100 4382 2346 5085 3094 2792 3670  243 2951 2445
## [127]  100 3420  274  100  912 3226 4552 5085 3329 2715 2013 4642  100 1252
## [141] 3403 5050 2149 3728 4559 4914 1552 4557 2456 4573 2445 3005 1123 3560
## [155] 4559 3154  680  530 4556 4573 2445  100 3473 3165 2327 4573  336 4831
## [169] 4411 3165 3226 2982 2412 3421  680 4577 3356 3233 2420 2696 1440 4797
## [183] 3456  381 4509 3819 2551 1561 3992 4642 2149 3005 3165 4559 1294 4642
## [197]  467 1129 2459 3891 4559 5057 3067 2982 1730  274 2667 3674  100 4928
## [211] 3165 2220 5108 2544 3665  274 3665 4900 1873 3005 4642 4808 2220 4482
## [225]  274 1394 4194 5025 4576  812  274 4976 2299 2036 4642 4559 1802  808
## [239] 2299 3676 2154 4925 3111 2022 4642  226 3165 2220 3226  590 2149 3395
## [253] 3189 3028 4593 3191 1549 2346 4559 2445 3678 3891 4925 3191 3165 3028
## [267] 1751  408 1873 2982 4552  100 1237 1873  100 2027 4629 3659 2723  533
## [281] 4418 2060 5050 3165 4629
##
## [[3]]
##   [1] 4559 3079 5100 3462 2445 3176 5025 2346 4559 4784 4299  364 2314 4937
##  [15] 3066 4194 3079 4556 4937 1094  481  158 4642 4937 1769 1873 2456 2696
##  [29]  100 4653 3165  274 3125 2934 3165 4806 3065 4642 1985 3231 1823 1188
##  [43] 2314 3111 2982 3176 1873 4597 3165 5101 1873 4990 3191 3598  100 1220
##  [57] 2528 2451 3614 1572 3375 4797  100 1774  590 4174 5103  248  467 3495
##  [71] 4559 3079 5100 3462  588 4025  336 2060 3193 4642 4291 5025 5025 3598
##  [85] 1338 3640 1930 4846 1486 4642 1337 1615 4559  521 1565  100 4120 3598
##  [99] 1930 4629 4642 4629 4577  336 3606 1572 4642 1394 1461  100  933  621
## [113]  274 4593  588  243 2804  100 3085 4293 3430 1873
{% endhighlight %}

Of course, not every review will contain the same number of words.
To deal with this we pick a particular referece length, truncating
longer sentences and padding shorter sentences with zeros. This is
all handled by the **keras** function `pad_sequences` (options
exist for whether truncation and/or padding is done as a prefix
or a suffix; the defaults are usually best unless you have a particular
reason for preferring an alternative).


{% highlight r %}
X <- pad_sequences(id, maxlen = 100)
X[1:5,]
{% endhighlight %}



{% highlight text %}
##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
## [1,]    0    0    0    0    0    0    0    0    0     0     0     0     0
## [2,] 3819 2551 1561 3992 4642 2149 3005 3165 4559  1294  4642   467  1129
## [3,] 4642 4937 1769 1873 2456 2696  100 4653 3165   274  3125  2934  3165
## [4,] 4559 3961 1401 1838 3233 4559  812 1572 1873  4806  4642  4728  5025
## [5,]    0 4976 4559 5007  567 4925  455 4800  270  1561  3123   108   270
##      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24]
## [1,]     0     0     0     0     0     0     0     0     0     0     0
## [2,]  2459  3891  4559  5057  3067  2982  1730   274  2667  3674   100
## [3,]  4806  3065  4642  1985  3231  1823  1188  2314  3111  2982  3176
## [4,]  4563  1538  4573  2445  1572  3669  1427  2346  4190  3165  4559
## [5,]  1519  1561  1102  4986   100  3127  4914  3197  4642  1696  1022
##      [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35]
## [1,]     0     0     0     0     0     0     0     0     0     0     0
## [2,]  4928  3165  2220  5108  2544  3665   274  3665  4900  1873  3005
## [3,]  1873  4597  3165  5101  1873  4990  3191  3598   100  1220  2528
## [4,]   812  4642  4399  3838  3408  3614  4962   684  3293  2445   270
## [5,]  4559   588  4925  4737  2420   270   289  1756  5025   270   226
##      [,36] [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46]
## [1,]     0     0   381  4190  3430  2299  5067  2696  4642  4732  3699
## [2,]  4642  4808  2220  4482   274  1394  4194  5025  4576   812   274
## [3,]  2451  3614  1572  3375  4797   100  1774   590  4174  5103   248
## [4,]  2412   809   274  2220  1802  3940  5025  2220  3490  4998  2445
## [5,]  4285  4212  3022  1930  4559  2700  3165  3849   274   940  4642
##      [,47] [,48] [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57]
## [1,]  2931  4988  2046    62  3229  3261  3258  3165    74  2299  1487
## [2,]  4976  2299  2036  4642  4559  1802   808  2299  3676  2154  4925
## [3,]   467  3495  4559  3079  5100  3462   588  4025   336  2060  3193
## [4,]  2168   274   138  3747  4962   874  1401   639  1572  3669  2687
## [5,]  3038   100  1774  2459   100  2167  1794   480  4559   812   336
##      [,58] [,59] [,60] [,61] [,62] [,63] [,64] [,65] [,66] [,67] [,68]
## [1,]  2754  3165   653  3818  3611  2299  3590  2076  4035  3983    70
## [2,]  3111  2022  4642   226  3165  2220  3226   590  2149  3395  3189
## [3,]  4642  4291  5025  5025  3598  1338  3640  1930  4846  1486  4642
## [4,]  4642  2220  3340  2187  4937  2748   100  2753  3165  2335  2346
## [5,]  1838   274   585  2399  3165  4190  1459  1072  1789  2299  2209
##      [,69] [,70] [,71] [,72] [,73] [,74] [,75] [,76] [,77] [,78] [,79]
## [1,]  3229  1040  3258  3165    48  3189  2456    10  3229   719  3258
## [2,]  3028  4593  3191  1549  2346  4559  2445  3678  3891  4925  3191
## [3,]  1337  1615  4559   521  1565   100  4120  3598  1930  4629  4642
## [4,]  2220  1536  1350  3840  3094  4629  2346  4559  4862  2420  4559
## [5,]  3693  4593  1794   100  4727   902  2346  1621  1420   100  1081
##      [,80] [,81] [,82] [,83] [,84] [,85] [,86] [,87] [,88] [,89] [,90]
## [1,]  3165     7  2445  2982  4161   680  2299  2696  2456  3125  2150
## [2,]  3165  3028  1751   408  1873  2982  4552   100  1237  1873   100
## [3,]  4629  4577   336  3606  1572  4642  1394  1461   100   933   621
## [4,]  1929  4341   274  1300  4559  1198  4559  2993   336  1899  4593
## [5,]  3165  4593  3026  3982   902  4623  5025  3165  4559  1873  4559
##      [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98] [,99] [,100]
## [1,]  2039  4642   100  3430  4978  2299  2149  2456  1873    622
## [2,]  2027  4629  3659  2723   533  4418  2060  5050  3165   4629
## [3,]   274  4593   588   243  2804   100  3085  4293  3430   1873
## [4,]  3001  4642   467  2346  2464   680  4322  4925   684   2456
## [5,]   521   289  1940  2663  1756  2299  2149  1620  5028   4642
{% endhighlight %}

The zero index is treated specially by the embedding, and always mapped
to the zero vector. Now we process the data as usual:


{% highlight r %}
y <- amazon$category
X_train <- X[amazon$train_id == "train",]
y_train <- to_categorical(y[amazon$train_id == "train"] - 1, num_classes = 3)
{% endhighlight %}

And construct a neural network with an embedding layer, a flatten
layer, and then dense and output layers.


{% highlight r %}
model <- keras_model_sequential()
model %>%
  layer_embedding(
    input_dim = length(vocab) + 1,
    output_dim = 50,
    input_length = ncol(X)
    ) %>%
  layer_flatten() %>%
  layer_dense(256) %>%
  layer_activation("relu")%>%
  layer_dense(ncol(y_train)) %>%
  layer_activation("softmax")
model
{% endhighlight %}



{% highlight text %}
## Model
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #
## ===========================================================================
## embedding_2 (Embedding)          (None, 100, 50)               255900
## ___________________________________________________________________________
## flatten_2 (Flatten)              (None, 5000)                  0
## ___________________________________________________________________________
## dense_3 (Dense)                  (None, 256)                   1280256
## ___________________________________________________________________________
## activation_3 (Activation)        (None, 256)                   0
## ___________________________________________________________________________
## dense_4 (Dense)                  (None, 3)                     771
## ___________________________________________________________________________
## activation_4 (Activation)        (None, 3)                     0
## ===========================================================================
## Total params: 1,536,927
## Trainable params: 1,536,927
## Non-trainable params: 0
## ___________________________________________________________________________
{% endhighlight %}

Finally, we


{% highlight r %}
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = "accuracy"
)

history <- model %>% fit(X_train, y_train, batch_size = 32,
              epochs = 10,
              validation_split = 0.1)
y_pred <- predict_classes(model, X) + 1
plot(history)
{% endhighlight %}

<img src="../assets/2017-11-14-class22/unnamed-chunk-17-1.png" title="plot of chunk unnamed-chunk-17" alt="plot of chunk unnamed-chunk-17" width="100%" />

The model performs reasonably well, though not yet a noticable
improvement over the elastic net models we built.


{% highlight r %}
tapply(y_pred == amazon$category - 1, amazon$train_id, mean)
{% endhighlight %}



{% highlight text %}
##        test       train       valid
##          NA 0.001666667 0.013896609
{% endhighlight %}

The problem is that while in the elastic net models we entirely
ignore word ordering, here we go all the way to the other extreme.
The model treats the sentence "I thought this movie was really good"
entirely different than "This movie was really good" because weights
in the dense layer apply specifically to each position of the word.
Using a 1-dimensional variation of convolutions will solve this
issue.

## Convolutions again

It turns out we can think of the output of the word embeddings
as being similar to the multidimensional tensors in image
processing.

For example, consider a word embedding with p equal to 3. We can see
this as three parallel 1-dimensional streams of length T,
much in the way that a color image is a 2d-dimensional combination
of parallel red, green, and blue channels.

In this context, we can apply 1-D constitutional layers
just as before: shared weights over some small kernel. Now, however,
the kernel has just a single spatial component. We also apply max
pooling to the sequence, with a window size of 10 (windows in one
dimension tend to be larger than in 2).


{% highlight r %}
model <- keras_model_sequential()
model %>%
  layer_embedding(
    input_dim = length(vocab) + 1,
    output_dim = 50,
    input_length = ncol(X)
    ) %>%

  layer_conv_1d(filters = 128, kernel_size = c(4)) %>%
  layer_max_pooling_1d(pool_size = 10L) %>%
  layer_dropout(rate = 0.5) %>%

  layer_flatten() %>%
  layer_dense(256) %>%
  layer_activation("relu")%>%
  layer_dense(ncol(y_train)) %>%
  layer_activation("softmax")

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = "accuracy"
)

model
{% endhighlight %}



{% highlight text %}
## Model
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #
## ===========================================================================
## embedding_3 (Embedding)          (None, 100, 50)               255900
## ___________________________________________________________________________
## conv1d_1 (Conv1D)                (None, 97, 128)               25728
## ___________________________________________________________________________
## max_pooling1d_1 (MaxPooling1D)   (None, 9, 128)                0
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 9, 128)                0
## ___________________________________________________________________________
## flatten_3 (Flatten)              (None, 1152)                  0
## ___________________________________________________________________________
## dense_5 (Dense)                  (None, 256)                   295168
## ___________________________________________________________________________
## activation_5 (Activation)        (None, 256)                   0
## ___________________________________________________________________________
## dense_6 (Dense)                  (None, 3)                     771
## ___________________________________________________________________________
## activation_6 (Activation)        (None, 3)                     0
## ===========================================================================
## Total params: 577,567
## Trainable params: 577,567
## Non-trainable params: 0
## ___________________________________________________________________________
{% endhighlight %}

Fitting this model on the data shows that it performs better than
the original embedding model (I believe it also bests all of the
models we constructed previously for the Amazon data).


{% highlight r %}
model %>% fit(X_train, y_train, batch_size = 32,
              epochs = 10,
              validation_split = 0.1)
y_pred <- predict_classes(model, X) + 1
tapply(y == y_pred, amazon$train_id, mean)
{% endhighlight %}



{% highlight text %}
##      test     train     valid
##        NA 0.9900000 0.9327404
{% endhighlight %}

Let's see if we can do better by including longer sequences
of text.


{% highlight r %}
X <- pad_sequences(id, maxlen = 300)
y <- amazon$category

X_train <- X[amazon$train_id == "train",]
y_train <- to_categorical(y[amazon$train_id == "train"] - 1, num_classes = 3)
{% endhighlight %}

I will replace the max pooling layer with a `layer_global_average_pooling_1d`.
This does pooling across all the nodes into one global number for each
filter. It is akin to what we did the VGG-16 transfer learning problem
where I took the maxium of each filter over the entire 7-by-7 grid.


{% highlight r %}
model <- keras_model_sequential()
model %>%
  layer_embedding(
    input_dim = length(vocab) + 1,
    output_dim = 50,
    input_length = ncol(X)
    ) %>%

  layer_conv_1d(filters = 128, kernel_size = c(5)) %>%
  layer_global_average_pooling_1d() %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(256) %>%
  layer_activation("relu")%>%
  layer_dense(ncol(y_train)) %>%
  layer_activation("softmax")

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = "accuracy"
)

model
{% endhighlight %}



{% highlight text %}
## Model
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #
## ===========================================================================
## embedding_4 (Embedding)          (None, 300, 50)               255900
## ___________________________________________________________________________
## conv1d_2 (Conv1D)                (None, 296, 128)              32128
## ___________________________________________________________________________
## global_average_pooling1d_1 (Glob (None, 128)                   0
## ___________________________________________________________________________
## dropout_2 (Dropout)              (None, 128)                   0
## ___________________________________________________________________________
## dense_7 (Dense)                  (None, 256)                   33024
## ___________________________________________________________________________
## activation_7 (Activation)        (None, 256)                   0
## ___________________________________________________________________________
## dense_8 (Dense)                  (None, 3)                     771
## ___________________________________________________________________________
## activation_8 (Activation)        (None, 3)                     0
## ===========================================================================
## Total params: 321,823
## Trainable params: 321,823
## Non-trainable params: 0
## ___________________________________________________________________________
{% endhighlight %}

Fitting this model shows a further, non-trivial improvment on the
model.


{% highlight r %}
model %>% fit(X_train, y_train, batch_size = 32,
              epochs = 6,
              validation_split = 0.1)
y_pred <- predict_classes(model, X) + 1
tapply(y == y_pred, amazon$train_id, mean)
{% endhighlight %}



{% highlight text %}
##      test     train     valid
##        NA 0.9751852 0.9538633
{% endhighlight %}

It should make sense that using more data perform better and that
the global position in the text of a sequence of words should not
matter much to the classifier.

## Visualize the word embedding

Once again, we can visulize the word embedding from this model.
There are over 5000 words though, making a full plot fairly noisy.
Instead, let's re-run the elastic net to find the terms the popped
out of the model as being important to the Amazon product classification
and look only at the projection of these.


{% highlight r %}
library(glmnet)
{% endhighlight %}



{% highlight text %}
## Warning: package 'glmnet' was built under R version 3.4.2
{% endhighlight %}



{% highlight r %}
token_list <- tokenize_words(amazon$text)
token_df <- term_list_to_df(token_list)
X <- term_df_to_matrix(token_df)
X_train <- X[amazon$train_id == "train",]
y_train <- y[amazon$train_id == "train"]
model_glmnet <- cv.glmnet(X_train, y_train, nfolds = 5, family = "multinomial")
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -65); Convergence for 65th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -63); Convergence for 63th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -63); Convergence for 63th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -63); Convergence for 63th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -63); Convergence for 63th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight r %}
beta <- coef(model_glmnet, s = model_glmnet$lambda.min)
beta <- Reduce(cbind, beta)

glmnet_words <- rownames(beta[apply(beta != 0, 1, any),][-1,])
glmnet_words
{% endhighlight %}



{% highlight text %}
##  [1] "the"        "of"         "one"        "book"       "my"
##  [6] "an"         "who"        "good"       "movie"      "film"
## [11] "story"      "these"      "read"       "see"        "characters"
## [16] "taste"      "series"     "books"      "flavor"     "use"
## [21] "dvd"        "reading"    "coffee"     "author"     "buy"
## [26] "product"    "movies"     "fun"        "novel"      "watch"
## [31] "written"    "try"        "price"      "show"       "writing"
## [36] "stories"    "sweet"      "chocolate"  "add"        "amazon"
## [41] "sugar"      "tried"      "acting"     "classic"    "watching"
## [46] "cup"        "box"        "eat"        "cast"       "reader"
## [51] "fresh"      "video"      "mix"        "bag"        "tastes"
## [56] "delicious"  "store"      "comedy"     "romance"    "milk"
## [61] "actors"     "drink"      "brand"      "healthy"    "snack"
## [66] "episode"    "tasty"      "flavors"    "watched"    "texture"
## [71] "expensive"  "tasting"    "tape"       "episodes"   "flick"
## [76] "flavored"   "workout"    "subscribe"  "ordering"   "salty"
## [81] "flavorful"  "bulk"       "bottles"    "klausner"
{% endhighlight %}

We'll grab the embedding layer from `model` and filter to
just those words in the elastic net model.


{% highlight r %}
embed <- model$get_layer(index = 1L)$get_weights()[[1]]
these <- match(glmnet_words, vocab)
{% endhighlight %}

Running PCA on the resulting data shows some interesting
patterns:


{% highlight r %}
library(irlba)
{% endhighlight %}



{% highlight text %}
## Warning: package 'irlba' was built under R version 3.4.2
{% endhighlight %}



{% highlight r %}
pca <- prcomp_irlba(embed[these,], n = 2)
df <- data_frame(pca1 = pca$x[,1],
                 pca2 = pca$x[,2],
                 bgram = glmnet_words)
ggplot(df, aes(pca1, pca2)) +
  geom_text(aes(label = bgram), size = 2) +
  theme_minimal()
{% endhighlight %}

<img src="../assets/2017-11-14-class22/unnamed-chunk-26-1.png" title="plot of chunk unnamed-chunk-26" alt="plot of chunk unnamed-chunk-26" width="100%" />

Notice that words associated with each category tend to
clump together in the PCA space. This is particularly
evident with the food data.











