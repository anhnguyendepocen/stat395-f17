---
title: "Class 16: Natural Language Processing"
author: "Taylor Arnold"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "../assets/2017-10-24-class16/")
knitr::opts_chunk$set(fig.height = 5)
knitr::opts_chunk$set(fig.width = 8.5)
knitr::opts_chunk$set(out.width = "100%")
knitr::opts_chunk$set(dpi = 300)
```

```{r, message = FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(tokenizers)
library(stringi)
library(smodels)
```

```{r, message = FALSE}
stylo <- read_csv("~/gh/ml_data/stylo_us.csv")
```


```{r, message = FALSE, warn = FALSE}
token_list <- tokenize_words(stylo$text)
token_df <- term_list_to_df(token_list)

X <- term_df_to_matrix(token_df, min_df = 0.03, max_df = 1,
                       scale = FALSE)

y <- stylo$author
X_train <- X[stylo$train_id == "train",]
X_valid <- X[stylo$train_id == "valid",]
y_train <- y[stylo$train_id == "train"]
y_valid <- y[stylo$train_id == "valid"]

library(glmnet)
model <- cv.glmnet(X_train, y_train, family = "multinomial")
```



```{r}
beta <- coef(model, s = model$lambda.1se)
beta <- Reduce(cbind, beta)
beta[apply(beta != 0, 1, any),]
```

```{r}
stylo$author_pred <- predict(model, newx = X, type = "class")
tapply(stylo$author_pred == stylo$author,
       stylo$train_id, mean)
```


## Authorship with character grams


```{r, message = FALSE, warn = FALSE}
token_list <- tokenize_character_shingles(stylo$text,
                                          n_min = 1, n = 5,
                                          strip_non_alphanum = FALSE)
token_df <- term_list_to_df(token_list)

X <- term_df_to_matrix(token_df, min_df = 0.03, max_df = 1,
                       scale = FALSE)

y <- stylo$author
X_train <- X[stylo$train_id == "train",]
X_valid <- X[stylo$train_id == "valid",]
y_train <- y[stylo$train_id == "train"]
y_valid <- y[stylo$train_id == "valid"]

library(glmnet)
model <- cv.glmnet(X_train, y_train, family = "multinomial")
```

```{r}
beta <- coef(model, s = model$lambda.1se)
beta <- Reduce(cbind, beta)
beta[apply(beta != 0, 1, any),]
```

```{r}
stylo$author_pred <- predict(model, newx = X, type = "class")
tapply(stylo$author_pred == stylo$author,
       stylo$train_id, mean)
```

```{r}
table(author_pred = stylo$author_pred,
      author = stylo$author)
```




## Visualization

```{r}
library(irlba)
X_pca <- prcomp_irlba(X_train, n = 2)$x
qplot(X_pca[,1], X_pca[,2], color = factor(y_train),
      alpha = I(0.3)) +
  viridis::scale_color_viridis(discrete = TRUE) +
  theme_minimal()
```

## SVD


$$ X = U \cdot D \cdot V^t $$


![](img/tm.jpg)


![](img/svd_fb.png)




```{r, message = FALSE}
library(irlba)
X_svd <- irlba::irlba(X_train, nv = 10)

names(X_svd)
length(X_svd$d)
dim(X_svd$u)
dim(X_svd$v)
```

```{r}
top_words <- apply(X_svd$v, 2, function(v) {
  id <- order(v, decreasing = TRUE)[1:5]
  stri_paste(colnames(X_train)[id], collapse = "; ")
})
top_words
```

```{r}
table(top_words[apply(X_svd$u, 1, which.max)], pnames[y_train])
```


