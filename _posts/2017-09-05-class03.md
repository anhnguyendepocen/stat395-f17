---
title: "Class 03: The Lady Tasting Tea"
author: "Taylor Arnold"
output: html_notebook
---




{% highlight r %}
library(readr)
library(ggplot2)
library(dplyr)
{% endhighlight %}

## Tea Reviews

Today we are going to fill in some of the gaps left from the
last class about building exploratory graphics and linear
models. The title for this class comes from an experiment
described in the 1935 text *The Design of Experiments* by
Sir Ronald Fisher, one of the founding fathers of modern
statistics.

We are going to look at a different dataset of teas. Specifically,
tea reviews from the [Adagio Tea](http://www.adagio.com/) website.
I collected this dataset about 12 months ago, so it should be
similar but not exactly the same as what is one the site today.
Let's read the data into R from my website as we have done with
the mammals and abalone datasets:


{% highlight r %}
tea <- read_csv("https://statsmaths.github.io/ml_data/tea.csv")
{% endhighlight %}

Opening the data in the data viewer, we see the required first
two columns and the response of interest: the average score
assigned to the tea by customers.

Variables available to predict the output are the type of tea,
the number of reviews received the price of the tea. The latter
is given in estimated cents per cup as reported on the site.
We also have the full name of the tea.

## Exploratory analysis

### Univariate plots

Before doing anything else, let's try to understand the distribution
of each of the variables.


{% highlight r %}
qplot(score, data = tea)
{% endhighlight %}



{% highlight text %}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
{% endhighlight %}

![plot of chunk unnamed-chunk-3](../assets/2017-09-05-class03/unnamed-chunk-3-1.png)

The score values are generally very high, with most of them above 88.
All of the scores are whole integers, and the most common values are
between 92 and 95.


{% highlight r %}
qplot(price, data = tea)
{% endhighlight %}



{% highlight text %}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
{% endhighlight %}

![plot of chunk unnamed-chunk-4](../assets/2017-09-05-class03/unnamed-chunk-4-1.png)

The price variable is heavily skewed, with a few very expensive teas.
Most are well under a quarter per cup.


{% highlight r %}
qplot(num_reviews, data = tea)
{% endhighlight %}



{% highlight text %}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
{% endhighlight %}

![plot of chunk unnamed-chunk-5](../assets/2017-09-05-class03/unnamed-chunk-5-1.png)

The number of reviews also have a bit of skew, but not nearly as
strongly as the price variable.


{% highlight r %}
qplot(type, data = tea)
{% endhighlight %}

![plot of chunk unnamed-chunk-6](../assets/2017-09-05-class03/unnamed-chunk-6-1.png)

There are twelve types of tea, with some having only a few samples
and others having over thirty.

### Bivariate plots

Now, we can proceed to bivariate plots showing the relationship
between each variable and the response.


{% highlight r %}
qplot(num_reviews, score, data = tea)
{% endhighlight %}

![plot of chunk unnamed-chunk-7](../assets/2017-09-05-class03/unnamed-chunk-7-1.png)

There seems to be a slight positive relationship between the number of
reviews and the score.


{% highlight r %}
qplot(price, score, data = tea)
{% endhighlight %}

![plot of chunk unnamed-chunk-8](../assets/2017-09-05-class03/unnamed-chunk-8-1.png)

Given the skew of the plot, it is hard to figure out the exact relationship
between price and score.

## Linear Models

Last time I used the `geom_smooth` function to put what I
called a best-fit line onto the plot. We then used the `lm`
function to estimate the slope and intercept of this line
numerically. Let's try to formalize this concept now.

The classical linear regression model assumes that the
average value, or mean, of the response Y is a linear
function of X. Symbolically, with the index i representing
the i'th sample, this gives:

$$ \text{mean} (Y_i) = \alpha + \beta * X_i $$

Similarly, we can write that Y is equal to a fixed linear
effect dependent on X plus a random variable epsilon with
zero mean.

$$ Y_i = \alpha + \beta * X_i + \epsilon_i, \quad mean(\epsilon_i) = 0 $$

The estimation task here is to find reasonable estimates for
the alpha and beta components given pairs of observations (X, Y).
There are many ways of doing this but by far the most common is
to use what is known as **Ordinary Least Squares** or OLS.
This selects the alpha and beta that minimize the squared errors
implied by the linear model. As before, let's write this down
symbolically:

$$ \alpha, \beta \in \arg\min \left\{ \left. \sum_i \left(y_i - a - b x_i \right)^2 \quad \right| \quad a, b \in \mathbb{R} \right\} $$

A natural question is to ask why we are interested in the squared
errors. We could just as likely ask for the minimizer of the
absolute value of the errors or the minimizer of the maximum
errors.

I can offer two justifications of the squared error, one numerical
and one statistical. Using the squared error, unlike the absolute
errors or maximum errors, produces a smooth function. That is, a
function that has an infinite number of derivatives at every point
(although, all derivatives after 2 are equal to zero). This makes it
easier to find a solution to the linear equation. Secondly, if the
errors are distributed as a normal random variable the OLS estimator
is equivalent to the maximum likelihood estimator (MLE).

## Linear Models - Visually

In order to better understand linear models, it helps to see a
picture. Below I have drawn a line through our dataset and
indicated the errors (also known as residuals) that the ordinary
least squares is concerned with minimizing. Note: don't worry
much about the code producing this graphic, concentrate just on
the output for now.


{% highlight r %}
tea$score_pred <- 89 + tea$num_reviews * 0.002
qplot(num_reviews, score, data = tea) +
  geom_line(aes(num_reviews, score_pred), color = "orange") +
  geom_segment(aes(xend = num_reviews, yend = score_pred), alpha = 0.5)
{% endhighlight %}

![plot of chunk unnamed-chunk-9](../assets/2017-09-05-class03/unnamed-chunk-9-1.png)

Notice that this line under-guesses most of the score of teas, particularly
if the number of reviews is low.

## Brute Force

How can we figure out what the best alpha and beta are for this
model? The most straightforward way would be to just try a large
number of values and see which one minimizes the response. This
is largely impractical but useful to see in this small example.
It will also let you see some more involved R code.

To start, we construct variables to hold the best sum of squares,
alpha, and beta values. We then create a double loop cycling
over all combinations of alpha and beta and testing whether the
given configuration gives a better sum of squares compared to
our current best value.


{% highlight r %}
best_ss <- Inf
best_a <- Inf
best_b <- Inf
for (a in seq(75, 120)) {
  for (b in seq(0.0001, 0.01, by = 0.0001)) {
    tea$score_pred <- a + tea$num_reviews * b
    ss <- sum((tea$score_pred - tea$score)^2, na.rm = TRUE)
    if (ss < best_ss) {
      best_a <- a
      best_b <- b
      best_ss <- ss
    }
  }
}
sprintf("best sum of squares=%03.02f at alpha=%03.01f and beta=%0.03f",
        best_ss, best_a, best_b)
{% endhighlight %}



{% highlight text %}
## [1] "best sum of squares=660.63 at alpha=92.0 and beta=0.001"
{% endhighlight %}

## Using lm

Now, let's compare the brute force algorithm with the lm function:


{% highlight r %}
model <- lm(score ~ num_reviews, data = tea)
model
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = score ~ num_reviews, data = tea)
## 
## Coefficients:
## (Intercept)  num_reviews  
##   9.225e+01    9.527e-04
{% endhighlight %}

Our brute-force solution is actually quote close to this! Of
course, it depended on me knowing approximately what the right
slope and intercept should be. In this case that was easy
graphically, but in more complex example it is a much more
challenging task.

Let's use the predict function again to fit this model
to our data:


{% highlight r %}
tea$score_pred <- predict(model, newdata = tea)
{% endhighlight %}

And now plot the fitted values. Does it visually
correspond to where you would expect the best fit
line to run?


{% highlight r %}
qplot(num_reviews, score, data = tea) +
  geom_point(aes(num_reviews, score_pred), color = "orange")
{% endhighlight %}

![plot of chunk unnamed-chunk-13](../assets/2017-09-05-class03/unnamed-chunk-13-1.png)

## Multivariate linear regression

The linear regression model I just introduced is known
as simple linear regresssion because there is only one
explanatory variable. We can easy consider multivariate
models; for instance, we can be write a two variable
model mathematically as follows:

$$ Y_i = \alpha + \beta * X_i + \gamma * Z_i + \epsilon_i, \quad mean(\epsilon_i) = 0 $$

The geometric interpretation of this is that we have plane
in place of the line in the simple linear regression model.

When I teach an applied statistics course, we spend a lot
of time working up to multivariate regression models. The
interpretation of multivariate models can quickly become
quite complex. However, using multivariate models in
statistical learning is much easier to understand: each
slope coefficient (beta and gamma here) corresponds to a
weight placed on how much the response changes with each
predictor variable.

Fitting multivariate models is also quite easy with the
`lm` function. Simply add the variables together that
you would like to use for prediction. Here we use both
the number of reviews and the price of the tea (from
now on, unless otherwise specified, we will only use
the training data to fit models):


{% highlight r %}
model <- lm(score ~ num_reviews + price, data = tea,
            subset = train_id == "train")
tea$score_pred <- predict(model, newdata = tea)
model
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = score ~ num_reviews + price, data = tea, subset = train_id == 
##     "train")
## 
## Coefficients:
## (Intercept)  num_reviews        price  
##   91.476718     0.001308     0.021595
{% endhighlight %}

The predictiveness of the model has been
greatly improved over the simple linear regression from
before. Next class we will see exactly how to quantify
this improvement.


