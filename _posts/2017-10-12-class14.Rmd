---
title: "Class 14: Chicago - Now With 80% More Crimes!"
author: "Taylor Arnold"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "../assets/2017-10-12-class14/")
```

```{r, message = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(methods)
library(keras)
```

```{r, message = FALSE}
crimes <- read_csv("~/files/ml_data/chi_crimes_12.csv")
```

$$ Y = \sigma(X \cdot A) \cdot B $$


```{r}
to_categorical(c(1,1,2,4,10))
```

```{r}
X <- scale(as.matrix(select(crimes, longitude, latitude, hour)))
y <- factor(crimes$crime_type)
crime_levels <- levels(y)
y <- as.integer(y) - 1

X_train <- X[crimes$train_id == "train",]
y_train <- to_categorical(y[crimes$train_id == "train"], num_classes = 12)
X_valid <- X[crimes$train_id == "valid",]
y_valid <- to_categorical(y[crimes$train_id == "valid"], num_classes = 12)
```

```{r}
y_train[sample(nrow(y_train), 10),]
```

```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 20, input_shape = c(3)) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 20) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 12) %>%
  layer_activation(activation = "softmax")
model
```


```{r}
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
```

```{r}
model %>%
  fit(X_train, y_train, epochs = 10,
      validation_data = list(X_valid, y_valid))
```

```{r}
crimes <- sample_frac(crimes, size = 1)
X <- scale(as.matrix(select(crimes, longitude, latitude, hour)))
y <- factor(crimes$crime_type)
crime_levels <- levels(y)
y <- as.integer(y) - 1

X_train <- X[crimes$train_id == "train",]
y_train <- to_categorical(y[crimes$train_id == "train"], num_classes = 12)
X_valid <- X[crimes$train_id == "valid",]
y_valid <- to_categorical(y[crimes$train_id == "valid"], num_classes = 12)
```

```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 20, input_shape = c(3)) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 20) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 12) %>%
  layer_activation(activation = "softmax")
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
```


```{r}
model %>%
  fit(X_train, y_train, epochs = 10,
      validation_data = list(X_valid, y_valid))
```

```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 20, input_shape = c(3)) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 20) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 12) %>%
  layer_activation(activation = "softmax")
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
```


```{r}
model %>%
  fit(X_train, y_train, epochs = 10,
      validation_data = list(X_valid, y_valid))
```

## Backpropagation

$$ \begin{align} \widehat{Y} &= \sigma(X \cdot A) \cdot B \\
                 \widehat{Y} &= Z \cdot B \end{align} \\
                 $$

It will be easier to index of $A$ as a vector, with elements $a_i$ rather than
having to keep track of the 2-dimensional nature of the matrix.

$$ \frac{\partial f}{\partial a_{i}} =
    \sum_k \frac{\partial z_{k}}{\partial a_{i}} \cdot \frac{\partial f}{\partial z_{k}} $$


$$ \frac{\partial f}{\partial z_{k}} =
    \sum_j \frac{\partial \widehat{y}_{j}}{\partial z_{k}} \cdot \frac{\partial f}{\partial \widehat{y}_{j}} $$


![](img/tikz40.png)







