---
title: "Class 10: Local Models"
author: "Taylor Arnold"
output: html_notebook
---





{% highlight r %}
library(dplyr)
library(ggplot2)
library(readr)
library(viridis)
library(methods)
{% endhighlight %}

## Income data

For the first half of today, we are going to look at the dataset
you used on your last lab.


{% highlight r %}
acs <- read_csv("https://statsmaths.github.io/ml_data/tract_median_income.csv")
{% endhighlight %}

Looking at the map, we see that income is highly local and highest
near large cities (such as NYC, Chicago, LA, and Houston).


{% highlight r %}
library(smodels)
temp <- filter(acs, train_id == "train", !(state %in% c("AK", "HI")))
qplot(lon, lat, data = temp, color = bin(median_income, 6),
      size = I(0.5), alpha = I(0.5)) +
  theme_minimal() +
  coord_map()
{% endhighlight %}

<img src="../assets/2017-09-28-class10/unnamed-chunk-3-1.png" title="plot of chunk unnamed-chunk-3" alt="plot of chunk unnamed-chunk-3" width="100%" />

While we have seen ways of building models that account for
linearites and interactions (GAMs and basis expansion), today's
focus will be on using inherantly local models to learn the
response variable here.

### Tuning k-nearest neighbors

The knn model, as we saw last time, requires us to use
a model matrix. We will construct one here using only
longitude and latitude:


{% highlight r %}
X <- as.matrix(select(acs, lon, lat))
y <- acs$median_income

X_train <- X[acs$train_id == "train",]
X_valid <- X[acs$train_id == "valid",]
y_train <- y[acs$train_id == "train"]
y_valid <- y[acs$train_id == "valid"]
{% endhighlight %}

While we usually do not need a specific validation X matrix
and response vector y, it will be useful here today. We will
fit the k-nearest neighbors algorithm for 25 values of the
hyperparameter k, specifically, the integers 1 through 25.
I will save the RMSE of the validation set for each value of
k.


{% highlight r %}
library(FNN)
{% endhighlight %}



{% highlight text %}
## Error in library(FNN): there is no package called 'FNN'
{% endhighlight %}



{% highlight r %}
rmse <- rep(NA, 25)
for (k in seq_along(rmse)) {
  y_valid_pred <- knn.reg(train = X_train, y = y_train,
                          test = X_valid, k = k)$pred
  rmse[k] <- sqrt( mean((y_valid_pred - y_valid)^2) )
}
{% endhighlight %}



{% highlight text %}
## Error in knn.reg(train = X_train, y = y_train, test = X_valid, k = k): could not find function "knn.reg"
{% endhighlight %}

With these values, we can plot the RMSE as a function of
the parameter k:


{% highlight r %}
qplot(seq_along(rmse), rmse) +
  geom_line() +
  theme_minimal()
{% endhighlight %}

<img src="../assets/2017-09-28-class10/unnamed-chunk-6-1.png" title="plot of chunk unnamed-chunk-6" alt="plot of chunk unnamed-chunk-6" width="100%" />

The optimal value seems to be at k equal to 5, with a drastic
degradation for k equal to 1 or 2.

One thing that may be counterintuitive about k-nearest neighbors
is the relationship between k and the model complexity. Here is
a plot of the fitted values for k equal to 3000 over the Northeastern
US:


{% highlight r %}
id <- (X_valid[,1] > -80) & (X_valid[,2] > 35)
pred_id <- knn.reg(train = X_train, y = y_train,
                   test = X_valid[id,], k = 3000)$pred
{% endhighlight %}



{% highlight text %}
## Error in knn.reg(train = X_train, y = y_train, test = X_valid[id, ], k = 3000): could not find function "knn.reg"
{% endhighlight %}



{% highlight r %}
df <- data_frame(lon = X_valid[id,1], lat = X_valid[id,2],
                 pred = pred_id)
{% endhighlight %}



{% highlight text %}
## Error in overscope_eval_next(overscope, expr): object 'pred_id' not found
{% endhighlight %}



{% highlight r %}
library(ggmap)
qmplot(lon, lat, data = df, color = bin(pred, 4)) +
  viridis::scale_color_viridis(discrete = TRUE) +
  ggtitle("k = 3000")
{% endhighlight %}



{% highlight text %}
## Error in data[, deparse(substitute(x))]: object of type 'closure' is not subsettable
{% endhighlight %}

There are large regions that all have very similar predicted
values. In contrast, here are the predicted value from k equal to
3.


{% highlight r %}
id <- (X_valid[,1] > -80) & (X_valid[,2] > 35)
pred_id <- knn.reg(train = X_train, y = y_train,
                   test = X_valid[id,], k = 3)$pred
{% endhighlight %}



{% highlight text %}
## Error in knn.reg(train = X_train, y = y_train, test = X_valid[id, ], k = 3): could not find function "knn.reg"
{% endhighlight %}



{% highlight r %}
df <- data_frame(lon = X_valid[id,1], lat = X_valid[id,2],
                 pred = pred_id)
{% endhighlight %}



{% highlight text %}
## Error in overscope_eval_next(overscope, expr): object 'pred_id' not found
{% endhighlight %}



{% highlight r %}
qmplot(lon, lat, data = df, color = bin(pred, 4)) +
  viridis::scale_color_viridis(discrete = TRUE) +
  ggtitle("k = 5")
{% endhighlight %}



{% highlight text %}
## Error in data[, deparse(substitute(x))]: object of type 'closure' is not subsettable
{% endhighlight %}

Here, the values change rapidly and there are many nearby
points with drastically different values. So, a high value
of k is a less complex model and a low value is a more
complex model. If this seems confusing, think about k being
the size of the entire training set (every point will then
be equal to a constant: the training set mean). Also perhaps
confusing is that models with a larger k take longer to fit;
this should make sense as there are more points to average,
but goes against our general idea that more complex models
are more difficult to fit.

## Clustering with K-means

An alternative to k-nearest neighbors is to split the dataset
into fixed neighborhoods of nearby points and use these as
indicator variables in a model. To do this, we need to cluster
the input data. There are many algorithms available for clustering;
one of the most widely known and implemented is k-means clustering.
This is achieved by way of the `kmeans` function and requires only
that we set the number of centers. Here is a plot of kmeans applied
to the `acs` data:


{% highlight r %}
set.seed(1)
acs$cluster_20 <- kmeans(X, centers = 20)$cluster
qmplot(lon, lat, data = acs, color = factor(cluster_20))
{% endhighlight %}



{% highlight text %}
## Warning: `panel.margin` is deprecated. Please use `panel.spacing` property
## instead
{% endhighlight %}

<img src="../assets/2017-09-28-class10/unnamed-chunk-9-1.png" title="plot of chunk unnamed-chunk-9" alt="plot of chunk unnamed-chunk-9" width="100%" />

Notice that I have set the random seed as the output of the algorithm
is stochastic. Here, longitude and latitude are on similar scales, but
generally you should scale the data matrix X prior to clustering.

Let's now build 20, 100, and 200 member clusters in the dataset:


{% highlight r %}
set.seed(1)
acs$cluster_20 <- kmeans(X, centers = 20)$cluster
acs$cluster_100 <- kmeans(X, centers = 100)$cluster
acs$cluster_200 <- kmeans(X, centers = 200)$cluster
{% endhighlight %}

We can use these in a `glmnet` model to determine which clusters
should have their own offsets. I am going to use the function
`model.Matrix` (note the captial M) from the **MatrixModels**
package for efficency purposes. We will discuss the details of
the function once we get to textual data:


{% highlight r %}
library(MatrixModels)
X <- model.Matrix(~ factor(cluster_20) + factor(cluster_100) +
                    factor(cluster_200) + -1 , data = acs, sparse = TRUE)
y <- acs$median_income
X_train <- X[acs$train_id == "train",]
y_train <- y[acs$train_id == "train"]
{% endhighlight %}

And we will fit an elastic net model to the data:


{% highlight r %}
library(glmnet)
{% endhighlight %}



{% highlight text %}
## Warning: package 'glmnet' was built under R version 3.4.2
{% endhighlight %}



{% highlight r %}
model <- cv.glmnet(X_train, y_train, alpha = 0.2)
acs$median_income_pred <- predict(model, newx = X)
{% endhighlight %}

Notice that the output resembles k-nearest neighbors:


{% highlight r %}
acs_ne <- filter(acs, lon > -80, lat > 35, lon < -30)
qmplot(lon, lat, data = acs_ne, color = bin(median_income_pred, 4)) +
  viridis::scale_color_viridis(discrete = TRUE) +
  ggtitle("glmnet")
{% endhighlight %}



{% highlight text %}
## Warning: `panel.margin` is deprecated. Please use `panel.spacing` property
## instead
{% endhighlight %}

<img src="../assets/2017-09-28-class10/unnamed-chunk-13-1.png" title="plot of chunk unnamed-chunk-13" alt="plot of chunk unnamed-chunk-13" width="100%" />

There are important differences with the k-nearest neighbors
method, however. The regions to group together have been determined
adaptively. Notice that upstate New York and Maine have moslty be
pushed into the same values. The Metro NYC area, in constrast, has
many more regions.

## Decision Trees

We will now look at a new dataset of housing prices in Ames,
Iowa. Unlike our other housing datasets though, the samples
here are individual houses.


{% highlight r %}
ames <- read_csv("https://statsmaths.github.com/ml_data/ames.csv")
{% endhighlight %}

One way to build a predictive algorithm is to describe a decision
tree. The best way to understand what this is is to see an example.
Here, we use the `tree` library (pre-installed in R) to predict
sales price:


{% highlight r %}
library(tree)
{% endhighlight %}



{% highlight text %}
## Error in library(tree): there is no package called 'tree'
{% endhighlight %}



{% highlight r %}
model <- tree(saleprice ~ neighborhood + overall_qual + overall_cond + year_built,
                      data = ames, subset = train_id == "train",
              control = tree.control(nobs = nrow(ames), mindev = 0.005))
{% endhighlight %}



{% highlight text %}
## Error in tree(saleprice ~ neighborhood + overall_qual + overall_cond + : could not find function "tree"
{% endhighlight %}

This relatively old library needs a special plotting notation (so
don't worry if this looks strange)


{% highlight r %}
par(mar = c(0,0,0,0))
plot(model, type = "uniform")
{% endhighlight %}



{% highlight text %}
## Warning in plot.xy(xy, type, ...): plot type 'uniform' will be truncated to
## first character
{% endhighlight %}



{% highlight text %}
## Error in plot.xy(xy, type, ...): invalid plot type 'u'
{% endhighlight %}

<img src="../assets/2017-09-28-class10/unnamed-chunk-16-1.png" title="plot of chunk unnamed-chunk-16" alt="plot of chunk unnamed-chunk-16" width="100%" />

{% highlight r %}
text(model, cex = 0.7, col = "purple")
{% endhighlight %}



{% highlight text %}
## Error in xy.coords(x, y, recycle = TRUE, setLab = FALSE): 'x' is a list, but does not have components 'x' and 'y'
{% endhighlight %}

In order to do prediction, we start at the top of the tree and
look at each logical statement. If True, move to the left and
if False move to right. At the bottom (the *terminal nodes* or *leaves*),
there are predicted values. The tree was built by greedily picking
variables to spit the dataset up by until some stopping criterion
was reached; this is usually a fixed depth, a fixed proportion of
the data, or a fixed decrease in the RMSE of accuracy rate.

## Random Forests

Decision trees give very noisy predictions due to their use of
greedy logic and because points on the boundary of a decision
cut-off are forced into a fixed bucket. By noisy, I mean that a
slightly different training set would yield significantly
different predictions for at least some of the test points.
This may seem like a design flaw, but we can easily turn it into
a design feature!

The idea of a random forest is to add some randomness into the
decision tree algorithm, fit a large number of trees using this
random variation, and produce predictions by averaging together
the predictions from all these individual trees (its a *forest*
because there are a lot of trees; get it?). The random logic
applies only the building of the trees; once created, each tree
is exactly the same as in the case above. The randomness comes
from two sources:

- for each tree, select only a subset of the training data to
train with
- for each split, select only a subset of the available variables
to split on

The exact values for these two random features can be set as
hyperparameters. We can fit random forests using the `randomForest`
function from the package with the same name as follows:


{% highlight r %}
library(randomForest)
{% endhighlight %}



{% highlight text %}
## Error in library(randomForest): there is no package called 'randomForest'
{% endhighlight %}



{% highlight r %}
set.seed(1)
model <- randomForest(saleprice ~ overall_qual + year_built,
                      data = ames, subset = train_id == "train",
                      ntree = 20, maxnodes = 3, mtry = 1)
{% endhighlight %}



{% highlight text %}
## Error in randomForest(saleprice ~ overall_qual + year_built, data = ames, : could not find function "randomForest"
{% endhighlight %}

Here I selected 20 randomly generated trees, each having at most
3 terminal nodes and only allowing one variable to be used at
each split. These are very low settings, used only for illustrating
the algorithm here. We can get predictions from each individual
tree by setting `predict.all` to `TRUE`:


{% highlight r %}
obj <- predict(model, newdata = ames, predict.all = TRUE)$individual
{% endhighlight %}



{% highlight text %}
## Error in predict.glmnet(object$glmnet.fit, newx, s = lambda, ...): You need to supply a value for 'newx'
{% endhighlight %}

Here is the prediction for just the third tree:


{% highlight r %}
ames$price_pred <- obj[,4]
{% endhighlight %}



{% highlight text %}
## Error in eval(expr, envir, enclos): object 'obj' not found
{% endhighlight %}



{% highlight r %}
qplot(overall_qual, year_built, data = ames, color = price_pred) +
  viridis::scale_color_viridis()
{% endhighlight %}



{% highlight text %}
## Error in FUN(X[[i]], ...): object 'price_pred' not found
{% endhighlight %}

<img src="../assets/2017-09-28-class10/unnamed-chunk-19-1.png" title="plot of chunk unnamed-chunk-19" alt="plot of chunk unnamed-chunk-19" width="100%" />

Can you figure out roughly what the tree looks like? It first
splits on overal quality being less than 7.5, and then splits
the lower quality houses by year built around 1982. The individual
prediction is not very smooth or complex.

Taking all of the twenty trees together, the average model
looks quite a bit different:


{% highlight r %}
ames$price_pred <- predict(model, newdata = ames)
{% endhighlight %}



{% highlight text %}
## Error in predict.glmnet(object$glmnet.fit, newx, s = lambda, ...): You need to supply a value for 'newx'
{% endhighlight %}



{% highlight r %}
qplot(overall_qual, year_built, data = ames, color = price_pred) +
  viridis::scale_color_viridis()
{% endhighlight %}



{% highlight text %}
## Error in FUN(X[[i]], ...): object 'price_pred' not found
{% endhighlight %}

<img src="../assets/2017-09-28-class10/unnamed-chunk-20-1.png" title="plot of chunk unnamed-chunk-20" alt="plot of chunk unnamed-chunk-20" width="100%" />

Helpfully, the **randomForest** also provides the function
`importance` that measures how important each variable is
to the model.


{% highlight r %}
importance(model)
{% endhighlight %}



{% highlight text %}
## Error in importance(model): could not find function "importance"
{% endhighlight %}

This is a measurement of how often the variable was used
in the model and how much it decreased the RMSE each time it
was used to split the dataset.

## Gradient Boosted Trees

Gradient boosted trees offer a slightly different approach
to random forests for making use of the noisy nature of
decision trees. Like random forests, they construct a
number of trees, each using only a subset of the training
data. They do not restrict the variables available for
each node to split on. Most importantly, gradient boosted
trees are fit in a sequence, with each tree trying to predict
the residuals left over by the other trees.

More exactly, if the fitted values from the t-th tree
are given by:

$$ \widehat{Y_i^t} $$
Then we train the k-th tree on the values Z given by:

$$ Z_i = Y_i - \eta \cdot \sum_{t = 1}^{k - 1} \widehat{Y_i^t} $$

The parameter eta is the learning rate. If set to one, this
is exactly fitting on the residuals of the prior trees.
Setting to less than one stop the trees from overfitting from
the first few trees. Here, we prepare a larger set of variables
from the `ames` dataset:


{% highlight r %}
X <- model.matrix(~ . -1 , data = ames[,-c(1:3)])
y <- ames$saleprice

y_train <- y[ames$train_id == "train"]
y_valid <- y[ames$train_id == "valid"]
X_train <- X[ames$train_id == "train",]
X_valid <- X[ames$train_id == "valid",]
{% endhighlight %}

We will use the **xgboost** package to fit gradient
boosted trees. I will set the eta parameter to 0.02.


{% highlight r %}
library(xgboost)
model <- xgboost(data = X_train, label = y_train,
                 max_depth = 2, eta = 0.01, nthread = 2,
                 nrounds = 10, objective = "reg:linear",
                 verbose = 1)
{% endhighlight %}



{% highlight text %}
## [1]	train-rmse:200618.734375 
## [2]	train-rmse:198737.156250 
## [3]	train-rmse:196874.984375 
## [4]	train-rmse:195032.078125 
## [5]	train-rmse:193208.218750 
## [6]	train-rmse:191403.156250 
## [7]	train-rmse:189616.750000 
## [8]	train-rmse:187848.890625 
## [9]	train-rmse:186099.203125 
## [10]	train-rmse:184367.687500
{% endhighlight %}

And we can do prediction on the dataset:


{% highlight r %}
y_valid_pred <- predict(model, newdata = X_valid)
sqrt(mean((y_valid - y_valid_pred)^2))
{% endhighlight %}



{% highlight text %}
## [1] 184145.2
{% endhighlight %}

Alternatively, we can use the function `xgb.DMatrix` to
combine the data matrix and labels:


{% highlight r %}
data_train <- xgb.DMatrix(data = X_train, label = y_train)
data_valid <- xgb.DMatrix(data = X_valid, label = y_valid)
{% endhighlight %}

And use a more advanced calling method for **xgboost**:


{% highlight r %}
watchlist <- list(train=data_train, valid=data_valid)

model <- xgb.train(data = data_train,
                 max_depth = 3, eta = 1, nthread = 2,
                 nrounds = 100, objective = "reg:linear",
                 watchlist = watchlist)
{% endhighlight %}



{% highlight text %}
## [1]	train-rmse:40330.535156	valid-rmse:43169.093750 
## [2]	train-rmse:33610.992188	valid-rmse:37997.203125 
## [3]	train-rmse:31372.427734	valid-rmse:36761.664062 
## [4]	train-rmse:29337.048828	valid-rmse:35631.214844 
## [5]	train-rmse:26596.851562	valid-rmse:33336.937500 
## [6]	train-rmse:25461.392578	valid-rmse:32947.140625 
## [7]	train-rmse:24650.820312	valid-rmse:32801.335938 
## [8]	train-rmse:23919.269531	valid-rmse:32419.238281 
## [9]	train-rmse:23095.589844	valid-rmse:31914.378906 
## [10]	train-rmse:22340.753906	valid-rmse:31895.162109 
## [11]	train-rmse:21651.023438	valid-rmse:31577.035156 
## [12]	train-rmse:21066.343750	valid-rmse:31433.369141 
## [13]	train-rmse:20407.320312	valid-rmse:31593.667969 
## [14]	train-rmse:20006.835938	valid-rmse:31484.441406 
## [15]	train-rmse:19412.910156	valid-rmse:31326.746094 
## [16]	train-rmse:18975.277344	valid-rmse:30915.310547 
## [17]	train-rmse:18436.187500	valid-rmse:30651.304688 
## [18]	train-rmse:18054.402344	valid-rmse:30396.039062 
## [19]	train-rmse:17787.878906	valid-rmse:30377.044922 
## [20]	train-rmse:17234.507812	valid-rmse:30251.289062 
## [21]	train-rmse:16929.322266	valid-rmse:30518.941406 
## [22]	train-rmse:16654.597656	valid-rmse:30842.927734 
## [23]	train-rmse:15975.585938	valid-rmse:30112.761719 
## [24]	train-rmse:15784.124023	valid-rmse:30092.865234 
## [25]	train-rmse:15675.307617	valid-rmse:30277.214844 
## [26]	train-rmse:15430.172852	valid-rmse:30436.361328 
## [27]	train-rmse:15219.816406	valid-rmse:30447.654297 
## [28]	train-rmse:14964.752930	valid-rmse:30278.925781 
## [29]	train-rmse:14784.914062	valid-rmse:30225.939453 
## [30]	train-rmse:14660.959961	valid-rmse:30495.990234 
## [31]	train-rmse:14255.576172	valid-rmse:30067.169922 
## [32]	train-rmse:14080.679688	valid-rmse:30131.451172 
## [33]	train-rmse:13779.409180	valid-rmse:30085.353516 
## [34]	train-rmse:13529.962891	valid-rmse:30170.121094 
## [35]	train-rmse:13356.772461	valid-rmse:30028.333984 
## [36]	train-rmse:13237.447266	valid-rmse:29971.148438 
## [37]	train-rmse:12931.975586	valid-rmse:29609.308594 
## [38]	train-rmse:12671.836914	valid-rmse:29725.718750 
## [39]	train-rmse:12468.765625	valid-rmse:29944.814453 
## [40]	train-rmse:12324.841797	valid-rmse:29862.779297 
## [41]	train-rmse:12126.733398	valid-rmse:29896.707031 
## [42]	train-rmse:11964.291992	valid-rmse:29762.478516 
## [43]	train-rmse:11729.118164	valid-rmse:29817.984375 
## [44]	train-rmse:11573.271484	valid-rmse:29779.437500 
## [45]	train-rmse:11448.891602	valid-rmse:29861.589844 
## [46]	train-rmse:11364.757812	valid-rmse:29846.423828 
## [47]	train-rmse:11233.819336	valid-rmse:30012.298828 
## [48]	train-rmse:11025.106445	valid-rmse:30001.486328 
## [49]	train-rmse:10941.583008	valid-rmse:29855.826172 
## [50]	train-rmse:10798.945312	valid-rmse:30025.759766 
## [51]	train-rmse:10668.367188	valid-rmse:30032.957031 
## [52]	train-rmse:10519.201172	valid-rmse:30134.927734 
## [53]	train-rmse:10324.464844	valid-rmse:30254.732422 
## [54]	train-rmse:10218.821289	valid-rmse:30159.515625 
## [55]	train-rmse:10059.325195	valid-rmse:30139.410156 
## [56]	train-rmse:9939.417969	valid-rmse:30298.671875 
## [57]	train-rmse:9891.927734	valid-rmse:30249.630859 
## [58]	train-rmse:9859.056641	valid-rmse:30229.199219 
## [59]	train-rmse:9804.065430	valid-rmse:30260.078125 
## [60]	train-rmse:9669.843750	valid-rmse:30391.789062 
## [61]	train-rmse:9573.946289	valid-rmse:30523.814453 
## [62]	train-rmse:9479.506836	valid-rmse:30308.539062 
## [63]	train-rmse:9400.970703	valid-rmse:30239.673828 
## [64]	train-rmse:9326.769531	valid-rmse:30217.699219 
## [65]	train-rmse:9255.462891	valid-rmse:30258.011719 
## [66]	train-rmse:9149.699219	valid-rmse:30270.216797 
## [67]	train-rmse:9052.831055	valid-rmse:30214.425781 
## [68]	train-rmse:8973.993164	valid-rmse:30175.757812 
## [69]	train-rmse:8890.920898	valid-rmse:30211.406250 
## [70]	train-rmse:8846.213867	valid-rmse:30175.837891 
## [71]	train-rmse:8696.657227	valid-rmse:30076.863281 
## [72]	train-rmse:8611.812500	valid-rmse:30036.369141 
## [73]	train-rmse:8551.380859	valid-rmse:30109.201172 
## [74]	train-rmse:8495.408203	valid-rmse:30147.087891 
## [75]	train-rmse:8432.862305	valid-rmse:30209.839844 
## [76]	train-rmse:8325.651367	valid-rmse:30163.521484 
## [77]	train-rmse:8263.044922	valid-rmse:30137.306641 
## [78]	train-rmse:8135.219238	valid-rmse:30089.601562 
## [79]	train-rmse:8062.696777	valid-rmse:30164.648438 
## [80]	train-rmse:8014.054688	valid-rmse:30159.554688 
## [81]	train-rmse:7971.409180	valid-rmse:30172.500000 
## [82]	train-rmse:7882.016113	valid-rmse:30133.130859 
## [83]	train-rmse:7804.717285	valid-rmse:30129.650391 
## [84]	train-rmse:7719.697266	valid-rmse:30125.693359 
## [85]	train-rmse:7651.633789	valid-rmse:30117.662109 
## [86]	train-rmse:7552.571777	valid-rmse:30185.779297 
## [87]	train-rmse:7471.630859	valid-rmse:30206.025391 
## [88]	train-rmse:7322.400391	valid-rmse:30278.060547 
## [89]	train-rmse:7227.331543	valid-rmse:30307.949219 
## [90]	train-rmse:7163.485840	valid-rmse:30325.591797 
## [91]	train-rmse:7091.191895	valid-rmse:30335.419922 
## [92]	train-rmse:7039.869141	valid-rmse:30367.908203 
## [93]	train-rmse:6911.497070	valid-rmse:30380.394531 
## [94]	train-rmse:6846.682617	valid-rmse:30364.867188 
## [95]	train-rmse:6813.285156	valid-rmse:30391.937500 
## [96]	train-rmse:6769.208984	valid-rmse:30383.072266 
## [97]	train-rmse:6682.001465	valid-rmse:30434.177734 
## [98]	train-rmse:6587.448242	valid-rmse:30431.578125 
## [99]	train-rmse:6518.343750	valid-rmse:30437.457031 
## [100]	train-rmse:6452.921875	valid-rmse:30333.429688
{% endhighlight %}

The algorithm is the same, but there are more options available
with `xgb.train`. As with random forests, there is a way of
looking at variable importance. I don't like the default output
view, so here is some code to make it look nicer:


{% highlight r %}
importance_matrix <- xgb.importance(model = model)
importance_matrix[,1] <- colnames(X)[as.numeric(importance_matrix[[1]]) + 1]
importance_matrix
{% endhighlight %}



{% highlight text %}
##                  Feature         Gain        Cover   Frequency
##   1:        overall_qual 6.983199e-01 5.396423e-02 0.047619048
##   2:         gr_liv_area 9.699202e-02 7.291202e-02 0.072916667
##   3:        bsmtfin_sf_1 5.855276e-02 6.468360e-02 0.047619048
##   4:          year_built 2.489881e-02 4.603058e-02 0.041666667
##   5:        `1st_flr_sf` 1.740830e-02 4.462527e-02 0.044642857
##  ---                                                          
## 134: mas_vnr_typeBrkFace 8.299826e-06 3.321259e-05 0.001488095
## 135: exterior_2ndHdBoard 7.674557e-06 6.227361e-06 0.001488095
## 136:      ms_subclass090 5.556382e-06 1.453051e-05 0.001488095
## 137:      ms_subclass045 3.952301e-06 1.037893e-05 0.001488095
## 138:           half_bath 1.579377e-06 1.660630e-05 0.001488095
{% endhighlight %}

## Thoughts on local models

We've covered a lot today. Here are some take aways:

- k-nearest neighbors are great for smoothing predictions
or creating meta-features
- k is inversely related to the complexity of the model
- using clusters simulated KNN but allows for covariates
and adaptation to the data
- don't use the `tree` function for actual predictions; I
only used it to illustrate decision trees
- random forests are easy to use and difficult to overfit
with
- gradient boosted trees are incredibly powerful (often
the give the most predictive models in large ML competitions)
- you need to tune eta and number of trees in GBT to get
a good model

The last point should be the object of study for the next
lab. We will discuss the details more next week when going
over the lab.

