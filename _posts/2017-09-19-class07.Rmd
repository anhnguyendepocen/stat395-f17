---
title: "Class 07: Up in the Air (with Matrices!)"
author: "Taylor Arnold"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "../assets/2017-09-19-class07/")
```

```{r, message = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
```

## Model Matricies Again



```{r}
X <- model.matrix(~ . -1 , data = ames[,-c(1:3)])
y <- ames$saleprice
train_id <- ames$train_id

y_train <- y[train_id == "train"]
X_train <- X[train_id == "train",]
```





### Solving Linear Models

Calculating the OLS solution for multivariate linear regression involves,
in matrix notation, finding the beta vector that minimizes the sum of
squared residules. We can write, in matrix notation, the residuals as
the following:

$$ r(\beta) = y - X \beta $$

And the sum of squared residuals can be derived as:

$$ \begin{align} \text{sum of squared residuals} &= r(\beta)^t r(\beta) \\
    &= \sum_{i = 1}^n r(\beta)_i \cdot r(\beta)_i
    \end{align} $$

This is a length n vector (or, equivalently, an n-by-1 matrix). The
optimization task can then be written compactly as:

$$ \begin{align} \widehat{\beta} &= \arg\min_b \left\{ r(b)^t r(b)  \right\} \\
      &= \arg\min_b \left\{ (y - X b)^t (y - X b)  \right\}
    \end{align} $$

To find the minimizer, we calculate the gradient and set it equal to
zero. This is not hard if you have learned how to work with gradients
(and also, remember what you learned):

$$ \begin{align} \nabla_b \left(r(b)^t r(b)\right)
   &= \nabla_b \left((y - X b)^t (y - X b)\right) \\
   &= \nabla_b \left(y^t y + b^t X^t X b - 2 b^t X^t y\right) \\
   &= 2 X^t X b - 2 X^t y.
  \end{align} $$

Setting this equal to zero, we derive what are known as the normal
equations (yes, this is plural; why?!):

$$ (X^t X) \cdot \widehat{\beta} = X^t y. $$

If the inverse exists, this gives an analytic formula for solving
the linear system of equations:

$$ \widehat{\beta} = (X^t X)^{-1} \cdot X^t y. $$

I could be even more formal about this and show that the Hessian matrix
is positive semi-definite everywhere and therefore we have found a global
minimum. Under the assumption that many of your are already overwhelmed
at this point, I'll skip this for today.
