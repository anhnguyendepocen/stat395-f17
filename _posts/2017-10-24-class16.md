---
title: "Class 16: All Hail the Chief"
author: "Taylor Arnold"
output: html_notebook
---




{% highlight r %}
library(readr)
library(dplyr)
library(ggplot2)
library(tokenizers)
library(stringi)
library(smodels)
{% endhighlight %}


{% highlight r %}
president <- read_csv("~/files/ml_data/presidents_3.csv")
{% endhighlight %}


{% highlight r %}
stri_wrap(president$text[1000], width = 60)
{% endhighlight %}



{% highlight text %}
##  [1] "Iraqis are showing their courage every day, and we are proud"
##  [2] "to be their allies in the cause of freedom. Our work in Iraq"
##  [3] "is difficult because our enemy is brutal. But that brutality"
##  [4] "has not stopped the dramatic progress of a new democracy."   
##  [5] "In less than 3 years, the nation has gone from dictatorship" 
##  [6] "to liberation, to sovereignty, to a Constitution, to"        
##  [7] "national elections. At the same time, our coalition has been"
##  [8] "relentless in shutting off terrorist infiltration, clearing" 
##  [9] "out insurgent strongholds, and turning over territory to"    
## [10] "Iraqi security forces."
{% endhighlight %}


{% highlight r %}
data(stop_words, package = "tidytext")
stop_words <- stop_words$word
sample(stop_words, 50)
{% endhighlight %}



{% highlight text %}
##  [1] "et"        "hereafter" "became"    "groups"    "without"  
##  [6] "awfully"   "got"       "men"       "doesn't"   "greetings"
## [11] "place"     "where"     "anything"  "seem"      "let"      
## [16] "enough"    "didn't"    "anything"  "ones"      "different"
## [21] "tried"     "goods"     "through"   "here's"    "among"    
## [26] "much"      "still"     "us"        "become"    "toward"   
## [31] "by"        "last"      "x"         "between"   "per"      
## [36] "state"     "but"       "further"   "these"     "new"      
## [41] "it'd"      "we'd"      "three"     "are"       "tell"     
## [46] "nine"      "gone"      "least"     "this"      "have"
{% endhighlight %}



{% highlight r %}
token_list <- tokenize_words(president$text,
                             stopwords = stop_words)
token_df <- term_list_to_df(token_list)

X <- term_df_to_matrix(token_df, min_df = 0.03, max_df = 0.97,
                       scale = TRUE)

y <- president$class
X_train <- X[president$train_id == "train",]
X_valid <- X[president$train_id == "valid",]
y_train <- y[president$train_id == "train"]
y_valid <- y[president$train_id == "valid"]

library(glmnet)
model <- cv.glmnet(X_train, y_train, family = "multinomial")
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -86); Convergence for 86th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -73); Convergence for 73th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -69); Convergence for 69th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -65); Convergence for 65th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -78); Convergence for 78th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -73); Convergence for 73th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -71); Convergence for 71th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -72); Convergence for 72th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -75); Convergence for 75th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -67); Convergence for 67th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight r %}
plot(model)
{% endhighlight %}

![plot of chunk unnamed-chunk-5](../assets/2017-10-24-class16/unnamed-chunk-5-1.png)


{% highlight r %}
beta <- coef(model, s = model$lambda[12])
length(beta)
{% endhighlight %}



{% highlight text %}
## [1] 3
{% endhighlight %}



{% highlight r %}
head(beta[[3]])
{% endhighlight %}



{% highlight text %}
## 6 x 1 sparse Matrix of class "dgCMatrix"
##                      1
## (Intercept) 0.03479369
## people      0.46827615
## america     .         
## american    .         
## americans   .         
## country     .
{% endhighlight %}


{% highlight r %}
for(j in 1:3) {
  beta <- coef(model, s = model$lambda[12])
  cat(c( "Obama", "Bush", "Clinton" )[j])
  cat(":\n")
  out <- paste(rownames(beta[[j]])[which((beta[[j]] != 0))], " (",
        sign(beta[[j]])[which((beta[[j]] != 0))], ")", sep = "")
  print(out)
  cat("\n")
}
{% endhighlight %}



{% highlight text %}
## Obama:
##  [1] "(Intercept) (-1)" "jobs (1)"         "future (1)"      
##  [4] "businesses (1)"   "job (1)"          "peace (-1)"      
##  [7] "hope (-1)"        "income (-1)"      "billion (-1)"    
## [10] "lead (1)"         "increase (-1)"    "research (1)"    
## [13] "invest (1)"       "afford (1)"       "finally (1)"     
## 
## Bush:
##  [1] "(Intercept) (1)"  "jobs (-1)"        "security (1)"    
##  [4] "deficit (-1)"     "freedom (1)"      "weapons (1)"     
##  [7] "college (-1)"     "social (1)"       "free (1)"        
## [10] "hope (1)"         "iraq (1)"         "terrorists (1)"  
## [13] "communities (-1)" "income (1)"       "billion (1)"     
## [16] "terror (1)"       "republicans (-1)"
## 
## Clinton:
##  [1] "(Intercept) (1)" "people (1)"      "energy (-1)"    
##  [4] "cut (1)"         "national (1)"    "welfare (1)"    
##  [7] "bill (1)"        "free (-1)"       "century (1)"    
## [10] "program (1)"     "crime (1)"       "service (1)"
{% endhighlight %}


{% highlight r %}
y_valid_pred <- predict(model, X_valid, type = "response",
                        s = model$lambda.min)[,,1]
head(y_valid_pred)
{% endhighlight %}



{% highlight text %}
##                1          2         3
## [1,] 0.119140063 0.56832098 0.3125390
## [2,] 0.075003795 0.61724273 0.3077535
## [3,] 0.371000864 0.09669129 0.5323078
## [4,] 0.004256906 0.01190841 0.9838347
## [5,] 0.302853653 0.18828918 0.5088572
## [6,] 0.183596798 0.27580156 0.5406016
{% endhighlight %}



{% highlight r %}
y_valid_pred <- predict(model, X_valid, type = "response",
                        s = model$lambda.min)[,,1]
y_valid_pred <- apply(y_valid_pred, 1, which.max)
mean(y_valid == y_valid_pred)
{% endhighlight %}



{% highlight text %}
## [1] 0.5732484
{% endhighlight %}

## Bigrams


{% highlight r %}
token_list <- tokenize_ngrams(president$text, n = 2, n_min = 1,
                             stopwords = stop_words)
token_df <- term_list_to_df(token_list)

X <- term_df_to_matrix(token_df, min_df = 0.03)
y <- president$class
X_train <- X[president$train_id == "train",]
X_valid <- X[president$train_id == "valid",]
y_train <- y[president$train_id == "train"]
y_valid <- y[president$train_id == "valid"]

library(glmnet)
model <- cv.glmnet(X_train, y_train, family = "multinomial")
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -71); Convergence for 71th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -68); Convergence for 68th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -74); Convergence for 74th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -70); Convergence for 70th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -74); Convergence for 74th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -70); Convergence for 70th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -70); Convergence for 70th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -64); Convergence for 64th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -73); Convergence for 73th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -80); Convergence for 80th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight r %}
plot(model)
{% endhighlight %}

![plot of chunk unnamed-chunk-10](../assets/2017-10-24-class16/unnamed-chunk-10-1.png)


{% highlight r %}
for(j in 1:3) {
  beta <- coef(model, s = model$lambda[15])
  cat(c( "Obama", "Bush", "Clinton" )[j])
  cat(":\n")
  out <- paste(rownames(beta[[j]])[which((beta[[j]] != 0))], " (",
        sign(beta[[j]])[which((beta[[j]] != 0))], ")", sep = "")
  print(out)
  cat("\n")
}
{% endhighlight %}



{% highlight text %}
## Obama:
##  [1] "(Intercept) (1)"    "country (-1)"       "jobs (1)"          
##  [4] "children (-1)"      "budget (-1)"        "education (1)"     
##  [7] "citizens (-1)"      "businesses (1)"     "job (1)"           
## [10] "deficit (1)"        "president (-1)"     "peace (-1)"        
## [13] "pass (-1)"          "hope (-1)"          "taxes (-1)"        
## [16] "strong (-1)"        "fellow (-1)"        "income (-1)"       
## [19] "billion (-1)"       "lead (1)"           "increase (-1)"     
## [22] "research (1)"       "administration (1)" "invest (1)"        
## [25] "afford (1)"         "progress (1)"       "expand (-1)"       
## [28] "finally (1)"        "republicans (1)"   
## 
## Bush:
##  [1] "(Intercept) (1)"     "american (-1)"       "jobs (-1)"          
##  [4] "families (-1)"       "education (-1)"      "future (-1)"        
##  [7] "citizens (1)"        "home (-1)"           "deficit (-1)"       
## [10] "money (1)"           "federal (1)"         "freedom (1)"        
## [13] "change (-1)"         "hard (-1)"           "weapons (1)"        
## [16] "continue (1)"        "college (-1)"        "life (1)"           
## [19] "free (1)"            "hope (1)"            "iraq (1)"           
## [22] "terrorists (1)"      "social security (1)" "cuts (-1)"          
## [25] "communities (-1)"    "military (1)"        "fight (1)"          
## [28] "income (1)"          "billion (1)"         "class (-1)"         
## [31] "terror (1)"          "coverage (1)"        "stand (-1)"         
## [34] "bipartisan (-1)"     "global (-1)"         "raise (-1)"         
## [37] "finally (-1)"        "republicans (-1)"   
## 
## Clinton:
##  [1] "(Intercept) (-1)" "people (1)"       "energy (-1)"     
##  [4] "united (-1)"      "cut (1)"          "president (1)"   
##  [7] "national (1)"     "change (1)"       "challenge (1)"   
## [10] "welfare (1)"      "law (-1)"         "pay (1)"         
## [13] "bill (1)"         "free (-1)"        "women (-1)"      
## [16] "parents (1)"      "protect (-1)"     "goal (-1)"       
## [19] "start (1)"        "trade (1)"        "century (1)"     
## [22] "program (1)"      "class (1)"        "democracy (1)"   
## [25] "crime (1)"        "stop (1)"         "research (-1)"   
## [28] "global (1)"       "proud (1)"        "expand (1)"      
## [31] "raise (1)"
{% endhighlight %}


{% highlight r %}
y_valid_pred <- predict(model, X_valid, type = "response",
                        s = model$lambda.min)[,,1]
y_valid_pred <- apply(y_valid_pred, 1, which.max)
mean(y_valid == y_valid_pred)
{% endhighlight %}



{% highlight text %}
## [1] 0.6019108
{% endhighlight %}

## Character grams


{% highlight r %}
token_list <- tokenize_character_shingles(president$text, n = 3,
                                          n_min = 1,
                                          strip_non_alphanum = TRUE)
token_df <- term_list_to_df(token_list)

X <- term_df_to_matrix(token_df, min_df = 0.03)
y <- president$class
X_train <- X[president$train_id == "train",]
X_valid <- X[president$train_id == "valid",]
y_train <- y[president$train_id == "train"]
y_valid <- y[president$train_id == "valid"]

library(glmnet)
model <- cv.glmnet(X_train, y_train, family = "multinomial")
plot(model)
{% endhighlight %}

![plot of chunk unnamed-chunk-13](../assets/2017-10-24-class16/unnamed-chunk-13-1.png)


{% highlight r %}
for(j in 1:3) {
  beta <- coef(model, s = model$lambda[12])
  cat(c( "Obama", "Bush", "Clinton" )[j])
  cat(":\n")
  out <- paste(rownames(beta[[j]])[which((beta[[j]] != 0))], " (",
        sign(beta[[j]])[which((beta[[j]] != 0))], ")", sep = "")
  print(out)
  cat("\n")
}
{% endhighlight %}



{% highlight text %}
## Obama:
## [1] "(Intercept) (-1)" "tha (1)"          "hat (1)"         
## [4] "mu (-1)"          "ats (1)"          "emu (-1)"        
## 
## Bush:
##  [1] "(Intercept) (1)" "hat (-1)"        "bu (-1)"        
##  [4] "ob (-1)"         "now (-1)"        "but (-1)"       
##  [7] "las (-1)"        "err (1)"         "ira (1)"        
## [10] "add (1)"         "oci (1)"        
## 
## Clinton:
## [1] "(Intercept) (-1)" "to (1)"           "all (1)"         
## [4] "ple (1)"          "tto (1)"          "htt (1)"         
## [7] "lfa (1)"          "ink (1)"          "ico (1)"
{% endhighlight %}


{% highlight r %}
y_valid_pred <- predict(model, X_valid, type = "response",
                        s = model$lambda.min)[,,1]
y_valid_pred <- apply(y_valid_pred, 1, which.max)
mean(y_valid == y_valid_pred)
{% endhighlight %}



{% highlight text %}
## [1] 0.6910828
{% endhighlight %}

## Negative examples

Let's redo the simple unigram term analysis:


{% highlight r %}
token_list <- tokenize_words(president$text,
                             stopwords = stop_words)
token_df <- term_list_to_df(token_list)

X <- term_df_to_matrix(token_df, min_df = 0.03, max_df = 0.97,
                       scale = FALSE)

y <- president$class
X_train <- X[president$train_id == "train",]
X_valid <- X[president$train_id == "valid",]
y_train <- y[president$train_id == "train"]
y_valid <- y[president$train_id == "valid"]

library(glmnet)
model <- cv.glmnet(X_train, y_train, family = "multinomial")
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -85); Convergence for 85th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -68); Convergence for 68th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -73); Convergence for 73th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -74); Convergence for 74th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -76); Convergence for 76th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -66); Convergence for 66th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -69); Convergence for 69th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -70); Convergence for 70th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -81); Convergence for 81th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -73); Convergence for 73th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -76); Convergence for 76th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}


{% highlight r %}
pnames <-c( "Obama", "Bush", "Clinton" )
table(actual = pnames[y_valid],
      pred = pnames[y_valid_pred])
{% endhighlight %}



{% highlight text %}
##          pred
## actual    Bush Clinton Obama
##   Bush      71      12    15
##   Clinton   11      71    26
##   Obama      9      24    75
{% endhighlight %}



{% highlight r %}
neg_example <- function(actual, pred) {
  pnames <-c( "Obama", "Bush", "Clinton" )
  ids <- which(y_valid == match(actual, pnames) &
               y_valid_pred == match(pred, pnames))
  cat(c(stri_wrap(president$text[sample(ids, 1)], width = 60),
    "\n"), sep = "\n")
}


neg_example("Bush", "Clinton")
{% endhighlight %}



{% highlight text %}
## For too long our culture has said, "If it feels good, do
## it." Now America is embracing a new ethic and a new creed,
## "Let's roll." In the sacrifice of soldiers, the fierce
## brotherhood of firefighters, and the bravery and generosity
## of ordinary citizens, we have glimpsed what a new culture of
## responsibility could look like. We want to be a nation that
## serves goals larger than self. We've been offered a unique
## opportunity, and we must not let this moment pass.
{% endhighlight %}



{% highlight r %}
neg_example("Bush", "Obama")
{% endhighlight %}



{% highlight text %}
## The question for everyone in this Chamber, running through
## every decision we make this year, is whether we are going
## to help or hinder this progress. For several years now,
## this town has been consumed by a rancorous argument over
## the proper size of the Federal Government. It's an important
## debate, one that dates back to our very founding. But when
## that debate prevents us from carrying out even the most
## basic functions of our democracy-;when our differences
## shut down Government or threaten the full faith and credit
## of the United States-;then we are not doing right by the
## American people. Now, as President, I'm committed to making
## Washington work better and rebuilding the trust of the
## people who sent us here.
{% endhighlight %}



{% highlight r %}
neg_example("Obama", "Bush")
{% endhighlight %}



{% highlight text %}
## This dramatic progress has brought its own challenge, in the
## rising costs of medical care and health insurance. Members
## of Congress, we must work together to help control those
## costs and extend the benefits of modern medicine throughout
## our country. Meeting these goals requires bipartisan effort,
## and 2 months ago, you showed the way. By strengthening
## Medicare and adding a prescription drug benefit, you kept
## a basic commitment to our seniors. You are giving them the
## modern medicine they deserve.
{% endhighlight %}

## Visualization


{% highlight r %}
library(irlba)
X_pca <- prcomp_irlba(X_train, n = 2)$x
qplot(X_pca[,1], X_pca[,2], color = factor(y_train),
      alpha = I(0.3)) +
  viridis::scale_color_viridis(discrete = TRUE) +
  theme_minimal()
{% endhighlight %}

![plot of chunk unnamed-chunk-19](../assets/2017-10-24-class16/unnamed-chunk-19-1.png)

## SVD


$$ X = U \cdot D \cdot V^t $$


![](img/tm.jpg)


![](img/svd_fb.png)





{% highlight r %}
library(irlba)
X_svd <- irlba::irlba(X_train, nv = 10)

names(X_svd)
{% endhighlight %}



{% highlight text %}
## [1] "d"     "u"     "v"     "iter"  "mprod"
{% endhighlight %}



{% highlight r %}
length(X_svd$d)
{% endhighlight %}



{% highlight text %}
## [1] 10
{% endhighlight %}



{% highlight r %}
dim(X_svd$u)
{% endhighlight %}



{% highlight text %}
## [1] 651  10
{% endhighlight %}



{% highlight r %}
dim(X_svd$v)
{% endhighlight %}



{% highlight text %}
## [1] 180  10
{% endhighlight %}


{% highlight r %}
top_words <- apply(X_svd$v, 2, function(v) {
  id <- order(v, decreasing = TRUE)[1:5]
  stri_paste(colnames(X_train)[id], collapse = "; ")
})
top_words
{% endhighlight %}



{% highlight text %}
##  [1] "people; health; care; america; american"    
##  [2] "health; care; insurance; tax; costs"        
##  [3] "health; people; care; world; insurance"     
##  [4] "people; government; welfare; tax; jobs"     
##  [5] "security; social; people; tax; benefits"    
##  [6] "tax; americans; people; american; america"  
##  [7] "children; america; tax; school; education"  
##  [8] "government; america; tax; jobs; world"      
##  [9] "country; government; americans; tax; nation"
## [10] "people; energy; america; tax; education"
{% endhighlight %}


{% highlight r %}
table(top_words[apply(X_svd$u, 1, which.max)], pnames[y_train])
{% endhighlight %}



{% highlight text %}
##                                              
##                                               Bush Clinton Obama
##   children; america; tax; school; education     22      19    17
##   country; government; americans; tax; nation   28      32    33
##   government; america; tax; jobs; world         24      27    33
##   health; care; insurance; tax; costs           10       9    12
##   health; people; care; world; insurance        17       4     1
##   people; energy; america; tax; education       22      19    20
##   people; government; welfare; tax; jobs        10      32    11
##   people; health; care; america; american       38      66    65
##   security; social; people; tax; benefits       42       7     7
##   tax; americans; people; american; america      8       6    10
{% endhighlight %}


