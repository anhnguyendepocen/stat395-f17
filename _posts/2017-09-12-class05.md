---
title: "Class 05: Up in the Air (with Matrices!)"
author: "Taylor Arnold"
output: html_notebook
---




{% highlight r %}
library(readr)
library(ggplot2)
library(dplyr)
{% endhighlight %}

## Matricies

### Linear Maps

Consider all functions from an n-dimensional space into an
m-dimensional space,

$$ f: \mathbb{R}^n \rightarrow \mathbb{R}^m, $$

That preserves addition,

$$ f(u + v) = f(u) + f(v) $$

And multiplication by a fixed scalar value,

$$ f(\alpha \cdot u) = \alpha \cdot f(u), \quad \alpha \in \mathbb{R}. $$

Such functions are known as **linear maps** (the definition can be abstracted
to infinite dimensional spaces and over other fields, but we will only need
the finite real-valued case in this course). 

There is an important representation theorem stating that every such mapping
can be described by an n-by-m array of real numbers. This array is commonly
called a **matrix**. How exactly do we calculate the function f given its
matrix representation A? It is easier to explain with fixed values for n
and m. Let's consider the following matrix:

$$ A = \left( \begin{array}{ccc} a_{1, 1} & a_{1, 2} & a_{1, 3} \\
                                 a_{2, 1} & a_{2, 2} & a_{2, 3} \end{array} \right) $$

So, in this case we have n = 3 and m = 2. Therefore this is a mapping that 
takes a triple of numbers and returns a pair of numbers. Let's define
the input as a vector u:

$$ u = \left( \begin{array}{c} u_{1} \\
                               u_{2} \\
                               u_{3} \end{array} \right) $$


And the output as a vector v:

$$ v = \left( \begin{array}{c} v_{1} \\
                               v_{2} \end{array} \right) $$

The linear map is then defined as:

$$ \begin{align} v_1 &= a_{1,1} \cdot u_{1} + a_{1,2} \cdot u_{2} + a_{1,3} \cdot u_{3} \\
                 v_2 &= a_{2,1} \cdot u_{1} + a_{2,2} \cdot u_{2} + a_{3,3} \cdot u_{3}
                 \end{align} $$

So, each component of v is a linear combination of all components of u. We can
write this compactly using summation notation:

$$ v_i = \sum_{j = 1}^m a_{i, j} \cdot u_j $$

Conveniently, this last equation holds for any arbitrary choice of n and m.
Finally, we represent this symbolically by

$$ Au = v. $$

### Function composition

Consider two linear maps:

$$ \begin{align} f: \mathbb{R}^n \rightarrow \mathbb{R}^m \\
                 g: \mathbb{R}^m \rightarrow \mathbb{R}^p \end{align} $$


If we apply f to an vector in n-dimensional space we get an
m-dimensional vector. We could then take the output of this
map and apply g to it in order to get a p-dimensional vector.
This is known a function composition. We can represent the
action of first applying f and then applying g as a new function
h:

$$ h = g(f(u)), \quad h: \mathbb{R}^n \rightarrow \mathbb{R}^p $$

It is a fairly intuitive and easy to prove result that if f and
g are linear maps, so is h. Let f be represented by the matrix A,
g by the matrix B, and h by the matrix C. A natural question is
what relationship exists between A, B, and C?

It turns out that the result is just another sum:

$$ C_{i, j} = \sum_{k = 1}^{m} A_{i, k} \cdot B_{k, j} $$

This is known as a **matrix product** and is written as:

$$ C = A \cdot B = AB $$

If the concept of matrix multiplication is new to you, an animated
visualization of multiplying two matricies can be useful. Of course,
for this semester understanding the abstract meaning behind matrix
multiplication (function composition) is much more important than
grasping the mechanics of computing the new matrix:

![](https://thumbs.gfycat.com/PositiveExhaustedAmericangoldfinch-size_restricted.gif)

Notice that if we represent vectors as one-column matricies, the
definition of matrix multiplication is equivalent to our defintion
of applying a matrix to a vector:

$$ Au = v. $$

## Matrices in R

### Basics

There is extensive functionality for working with matricies in
R. Let's make a random 5-by-5 matrix and a 5-by-matrix, the 
latter we can think of as a vector.


{% highlight r %}
A <- matrix(sample(1:99, 25), 5, 5)
A
{% endhighlight %}



{% highlight text %}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]   47   90   30   31   94
## [2,]   84   50   51   23   36
## [3,]   71   13   86   95   53
## [4,]   61   18   29   83   39
## [5,]   92   82   58   67    1
{% endhighlight %}



{% highlight r %}
b <- matrix(sample(1:99, 5))
b
{% endhighlight %}



{% highlight text %}
##      [,1]
## [1,]   33
## [2,]   41
## [3,]   73
## [4,]    8
## [5,]   36
{% endhighlight %}

Element-wise arithmetic is assumed by default:


{% highlight r %}
A + A
{% endhighlight %}



{% highlight text %}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]   94  180   60   62  188
## [2,]  168  100  102   46   72
## [3,]  142   26  172  190  106
## [4,]  122   36   58  166   78
## [5,]  184  164  116  134    2
{% endhighlight %}

To calculate a matrix produce, R requires us to use the
symbol `%*%`. If a matrix is square, there is nothing
stopping us from composing a matrix with itself:


{% highlight r %}
A %*% A
{% endhighlight %}



{% highlight text %}
##       [,1]  [,2]  [,3]  [,4]  [,5]
## [1,] 22438 17386 14931 15248 10551
## [2,] 16484 14089 12211 12920 13332
## [3,] 21206 14214 16018 22106 15458
## [4,] 15089 11459  9911 14562 11195
## [5,] 19509 14422 13931 15876 17288
{% endhighlight %}

Similarly, we can multiply the matrix by the column vector
to compute the action of the matrix A as a linear map:


{% highlight r %}
A %*% b
{% endhighlight %}



{% highlight text %}
##       [,1]
## [1,] 11063
## [2,] 10025
## [3,] 11822
## [4,]  6936
## [5,] 11204
{% endhighlight %}

The transpose of a matrix, denoted by using t as a superscript,
can be computed with the t function:


{% highlight r %}
t(A)
{% endhighlight %}



{% highlight text %}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]   47   84   71   61   92
## [2,]   90   50   13   18   82
## [3,]   30   51   86   29   58
## [4,]   31   23   95   83   67
## [5,]   94   36   53   39    1
{% endhighlight %}

This is often useful in matrix computations. Similarly, we
can computing the inverse of a matrix with the function
`solve`:


{% highlight r %}
solve(A)
{% endhighlight %}



{% highlight text %}
##               [,1]         [,2]         [,3]         [,4]         [,5]
## [1,] -0.0079853935  0.021713582 -0.010928228  0.014205007 -0.005861181
## [2,]  0.0079273585 -0.010224224 -0.002166503 -0.007022786  0.011613703
## [3,]  0.0002671381 -0.002710032  0.018696460 -0.023642234  0.003584944
## [4,]  0.0009323966 -0.015027380  0.001456184  0.009495853  0.005824360
## [5,]  0.0066482212  0.004753101  0.001091240  0.004035223 -0.011253843
{% endhighlight %}

A matrix inverse, by definition, describes the inverse of the underlying
linear map (its also relatively simple to show that linear maps have 
linear inverses, when they exist). Note that matrix inversion is a
computationally unstable procedure and should be avoided when possible.

### Subsetting matricies

It will often be useful to take a subst of the rows and
columns in a matrix. Here we take rows 2 and 3 and columns
1 and 2:


{% highlight r %}
A[2:3, 1:2]
{% endhighlight %}



{% highlight text %}
##      [,1] [,2]
## [1,]   84   50
## [2,]   71   13
{% endhighlight %}

Here we take columns 1 and 2; by leaving the rows 
component empty every row is returned:


{% highlight r %}
A[,1:2]
{% endhighlight %}



{% highlight text %}
##      [,1] [,2]
## [1,]   47   90
## [2,]   84   50
## [3,]   71   13
## [4,]   61   18
## [5,]   92   82
{% endhighlight %}

There is a strange convention in R that, by default, if we
select a sub-matrix with only one row or one column, the
result will be converted from a rectangular matrix to a
non-dimensional vector. Notice that the output below is
not given as a matrix with one column:


{% highlight r %}
A[,1]
{% endhighlight %}



{% highlight text %}
## [1] 47 84 71 61 92
{% endhighlight %}

Usually this is not a problem, but if we want to be safe
we can add the option `drop = FALSE` to the subset command.
Notice the difference in the output here compared to the
output above:


{% highlight r %}
A[,1,drop = FALSE]
{% endhighlight %}



{% highlight text %}
##      [,1]
## [1,]   47
## [2,]   84
## [3,]   71
## [4,]   61
## [5,]   92
{% endhighlight %}

Finally, we can also subset by giving a logical statement
in either the rows or columns spot. Here we select only those
rows where the numbers 1 through 5 are greater than 3:


{% highlight r %}
A[1:5 > 3,]
{% endhighlight %}



{% highlight text %}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]   61   18   29   83   39
## [2,]   92   82   58   67    1
{% endhighlight %}

As expected, the output is a matrix with just two rows.

## Multivariate Linear Models

### Matrix Formulation

We have been working with multivariate linear models over the
past few classes, though I have only ever written the formal
equation in the case where there are two explanatory variables.
In general, multivariate regression represents the following
model:

$$y_i = x_{1,i} \beta_1 + x_{2,i} \beta_2 + \cdots + x_{1,p} \beta_p + \epsilon_i$$

For simplicity, we won't include an explicit intercept term in 
the model. If we want one, we will just make the first variable $x_{1,i}$
equal to one for every value of i.

The statistical estimation problem is to estimate the p
components of the multivariate vector beta.

Using our matrix notation, we can write the linear model
simultaneously for all observations:

$$ \left(\begin{array}{c}y_1\\ y_2\\ \vdots\\ y_n\end{array}\right) =
  \left(\begin{array}{cccc}x_{1,1}&x_{2,1}&\cdots&x_{p,1}\\
                           x_{1,2}&\ddots&&x_{p,2}\\
                           \vdots&&\ddots&\vdots\\
                           x_{1,n}&x_{2,n}&\cdots&x_{p,n}\\\end{array}\right)
  \left(\begin{array}{c}\beta_1\\ \beta_2\\ \vdots\\ \beta_p\end{array}\right) +
  \left(\begin{array}{c}\epsilon_1\\ \epsilon_2\\ \vdots\\ \epsilon_n\end{array}\right) $$


Which can be compactly written as:

$$ y = X \beta + \epsilon $$

The matrix X is known as the **design matrix** or **model matrix**.
For reference, note the following equation yields these dimensions:

$$ y \in \mathbb{R}^n $$
$$ X \in \mathbb{R}^{n \times p} $$
$$ \beta \in \mathbb{R}^p $$
$$ \epsilon \in \mathbb{R}^n $$

### Solving Linear Models

Calculating the OLS solution for multivariate linear regression involves,
in matrix notation, finding the beta vector that minimizes the sum of 
squared residules. We can write, in matrix notation, the residuals as
the following:

$$ r(\beta) = y - X \beta $$

And the sum of squared residuals can be derived as:

$$ \begin{align} \text{sum of squared residuals} &= r(\beta)^t r(\beta) \\
    &= \sum_{i = 1}^n r(\beta)_i \cdot r(\beta)_i
    \end{align} $$

This is a length n vector (or, equivalently, an n-by-1 matrix). The
optimization task can then be written compactly as:

$$ \begin{align} \widehat{\beta} &= \arg\min_b \left\{ r(b)^t r(b)  \right\} \\
      &= \arg\min_b \left\{ (y - X b)^t (y - X b)  \right\}
    \end{align} $$

To find the minimizer, we calculate the gradient and set it equal to
zero. This is not hard if you have learned how to work with gradients
(and also, remember what you learned):

$$ \begin{align} \nabla_b \left(r(b)^t r(b)\right)  
   &= \nabla_b \left((y - X b)^t (y - X b)\right) \\
   &= \nabla_b \left(y^t y + b^t X^t X b - 2 b^t X^t y\right) \\
   &= 2 X^t X b - 2 X^t y.
  \end{align} $$

Setting this equal to zero, we derive what are known as the normal
equations (yes, this is plural; why?!):

$$ (X^t X) \cdot \widehat{\beta} = X^t y. $$

If the inverse exists, this gives an analytic formula for solving
the linear system of equations:

$$ \widehat{\beta} = (X^t X)^{-1} \cdot X^t y. $$

I could be even more formal about this and show that the Hessian matrix
is positive semi-definite everywhere and therefore we have found a global
minimum. Under the assumption that many of your are already overwhelmed
at this point, I'll skip this for today.

## Linear Models with Matrices in R

Let's now try to apply these theoretical ideas to an actual dataset.
I'll load in the flights data from my website:


{% highlight r %}
flights <- read_csv("~/files/ml_data/flights_15min.csv")
{% endhighlight %}

Note that the flights data, like every other dataset we have used,
is not a matrix. It is something that R calls a **data frame**:


{% highlight r %}
class(flights)
{% endhighlight %}



{% highlight text %}
## [1] "tbl_df"     "tbl"        "data.frame"
{% endhighlight %}

While both matricies and data frames have data organized in rows and
columns, matricies force all of the data to be numeric (its actually
more complicated than this in R, but just go along with it for now).
A data frame on the other hand can have different variable types in
each column.

An easy way to construct a matrix in R, is to use the `select` function
to grab only numeric columns and the function `as.matrix` to convert
the output into a matrix object. We will throughout the course use the
notation that the variable y is a vector holding the response of 
interest and X is a matrix containing the variables we want to use in
a model.


{% highlight r %}
y <- flights$delayed

X <- as.matrix(select(flights, arr_hour, dep_hour))
X[1:10,]
{% endhighlight %}



{% highlight text %}
##       arr_hour dep_hour
##  [1,]       22       21
##  [2,]       13       11
##  [3,]       22       21
##  [4,]       22       20
##  [5,]       14       12
##  [6,]       16        8
##  [7,]       21       15
##  [8,]       19       17
##  [9,]       12        9
## [10,]       12       11
{% endhighlight %}

We can then create specific training and validation sets
using the logical subset method from above:


{% highlight r %}
X_train <- X[flights$train_id == "train", ]
X_valid <- X[flights$train_id == "valid", ]
y_train <- y[flights$train_id == "train"]
y_valid <- y[flights$train_id == "valid"]
{% endhighlight %}

From here, we can compute the regression vector beta:


{% highlight r %}
beta <- solve(t(X_train) %*% X_train) %*% t(X_train) %*% y_train
beta
{% endhighlight %}



{% highlight text %}
##               [,1]
## arr_hour 0.0522682
## dep_hour 0.0531534
{% endhighlight %}

So this generally looks good, but we left out the intercept. Recall
that our matrix formulation required us to add an explicit column of
1's if we wanted an intercept. Let's do this directly in the matrix
X using the `cbind` function, and repeat:


{% highlight r %}
X <- as.matrix(select(flights, arr_hour, dep_hour))
X <- cbind(1, X)
X_train <- X[flights$train_id == "train", ]
X_valid <- X[flights$train_id == "valid", ]

beta <- solve(t(X_train) %*% X_train) %*% t(X_train) %*% y_train
beta
{% endhighlight %}



{% highlight text %}
##                [,1]
##          0.20905417
## arr_hour 0.04406644
## dep_hour 0.04824098
{% endhighlight %}

Perfect, now we have a regression vector. To do prediction,
we just calculate X times beta:


{% highlight r %}
y_valid_pred <- X_valid %*% beta
sqrt(mean( (y_valid - y_valid_pred)^2))
{% endhighlight %}



{% highlight text %}
## [1] 2.415901
{% endhighlight %}

Let's verify that this gives the same output as the `lm` function:


{% highlight r %}
model <- lm(delayed ~ arr_hour +  dep_hour, data = flights,
            subset = train_id == "train")
model
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = delayed ~ arr_hour + dep_hour, data = flights, subset = train_id == 
##     "train")
## 
## Coefficients:
## (Intercept)     arr_hour     dep_hour  
##     0.20905      0.04407      0.04824
{% endhighlight %}

And, as hoped, it does!

## Using lm.fit

We have seen how to use the `lm` function to quickly fit
linear regression directly from data frames. Now, we also
know how to do this directly with matrix operations. There
is an intermediate function that solves the linear regression
problem directly from our matricies called `lm.fit`. In 
fact, the `lm` function internally calls this function.
As inputs, it takes just the X matrix and response y. There
is a lot of diagnostic output, but we will take just the 
`coef` component, corresponding to the coefficents matrix:


{% highlight r %}
lm.fit(X_train, y_train)$coef
{% endhighlight %}



{% highlight text %}
##              arr_hour   dep_hour 
## 0.20905417 0.04406644 0.04824098
{% endhighlight %}

This, once again, aligns with the output from the other
two methods.

## Building model matricies

When we simply want to set the model matrix X to a subset
of the numeric columns in our data frame, the function
`as.matrix` is usually sufficent. The formula interface
to `lm` is incredibly useful however when using categorical
variables or when processing numeric variables by special
functions such as `poly`.

The function `model.matrix` allows us to compute the model
matrix from the formula interface. In fact, the `lm` function
calls this to convert our inputs into a model matrix. The
output of this is then passed to `lm.fit`. It will also, by
default, include an intercept term for us. Here we use it
to build a model matrix of the carriers in the flights data:


{% highlight r %}
X <- model.matrix(~ carrier , data = flights)
head(X)
{% endhighlight %}



{% highlight text %}
##   (Intercept) carrierAA carrierAQ carrierAS carrierB6 carrierCO carrierDL
## 1           1         0         0         0         0         0         0
## 2           1         0         0         0         0         0         0
## 3           1         0         0         0         0         0         0
## 4           1         0         0         0         0         0         0
## 5           1         0         0         0         0         0         0
## 6           1         0         0         0         1         0         0
##   carrierEV carrierF9 carrierFL carrierHA carrierMQ carrierNW carrierOH
## 1         0         0         0         0         0         0         0
## 2         1         0         0         0         0         0         0
## 3         0         0         0         0         0         1         0
## 4         0         0         0         0         0         0         0
## 5         0         0         0         0         0         0         0
## 6         0         0         0         0         0         0         0
##   carrierOO carrierUA carrierUS carrierWN carrierXE carrierYV
## 1         0         0         0         0         0         1
## 2         0         0         0         0         0         0
## 3         0         0         0         0         0         0
## 4         0         0         0         1         0         0
## 5         0         0         1         0         0         0
## 6         0         0         0         0         0         0
{% endhighlight %}

Notice that each of these new variables is an indicator for
whether a given flight was flown on the associated carrier.
Let's look at the first 6 carriers to verify this:


{% highlight r %}
head(flights$carrier)
{% endhighlight %}



{% highlight text %}
## [1] "YV" "EV" "NW" "WN" "US" "B6"
{% endhighlight %}

If we look at a table of all the carriers, we see that one
of the carriers is missing:


{% highlight r %}
table(flights$carrier)
{% endhighlight %}



{% highlight text %}
## 
##   9E   AA   AQ   AS   B6   CO   DL   EV   F9   FL   HA   MQ   NW   OH   OO 
##  748 2247   95  555  653 1024 1575 1023  345  817  117 1785 1635  873 2053 
##   UA   US   WN   XE   YV 
## 1701 1839 3708 1267  940
{% endhighlight %}

The missing value "9E" is known as the **baseline** in this model.
The intercept indicates the expected delay of flights on this carrier.
All of the other terms give how much more or less each carrier is
to be delayed relative to the baseline "9E".

Changing the baseline will change all of the beta coefficents.
However, the predicted values will remain the same. In 209 and 289,
I spend a lot of time talking about changing the baseline and
understanding the model from different perspectives. As we are
focused on prediction, which is unchanged, this will be much less
of a concern for us. Just note that by default variables are sorted
in alphabetical order, which is why "9E" is the baseline here.

With this model matrix, we can compute the regression vector
by first creating training and validation matricies:


{% highlight r %}
y_train <- y[flights$train_id == "train"]
y_valid <- y[flights$train_id == "valid"]
X_train <- X[flights$train_id == "train",]
X_valid <- X[flights$train_id == "valid",]
{% endhighlight %}

And then using `lm.fit`:


{% highlight r %}
beta <- lm.fit(X_train, y_train)$coef
beta
{% endhighlight %}



{% highlight text %}
## (Intercept)   carrierAA   carrierAQ   carrierAS   carrierB6   carrierCO 
##  1.45336226  0.45833410 -0.99573514  0.19141386  0.30913774  0.27300590 
##   carrierDL   carrierEV   carrierF9   carrierFL   carrierHA   carrierMQ 
## -0.19697928  0.91240956 -0.68686479 -0.02243442 -0.94686875  0.45032046 
##   carrierNW   carrierOH   carrierOO   carrierUA   carrierUS   carrierWN 
## -0.20260083  0.30558313 -0.13428070  0.19623614  0.06097466 -0.39594176 
##   carrierXE   carrierYV 
##  0.52541758  0.23031121
{% endhighlight %}

## Sparse Matricies

You may have noticed that the design matrix above contains
primarily zeros. With large datasets and even more categories,
we would be wasting a lot of valuable memory holding all of
these values. R has support for objects known as sparse
matricies. These store only the non-zero elements, saving
space when most elements are zero. To access them, we'll load
the **MatrixModels** package and using the function `model.Matrix`
(in general, in R capital letters indicate sparse matrix 
variations of dense matrix operations).


{% highlight r %}
library(methods)
library(MatrixModels)
X <- model.Matrix(~ carrier , data = flights[,-c(1:3)], sparse = TRUE)
X[1:10,]
{% endhighlight %}



{% highlight text %}
## 10 x 20 sparse Matrix of class "dgCMatrix"
{% endhighlight %}



{% highlight text %}
##    [[ suppressing 20 column names '(Intercept)', 'carrierAA', 'carrierAQ' ... ]]
{% endhighlight %}



{% highlight text %}
##                                           
## 1  1 . . . . . . . . . . . . . . . . . . 1
## 2  1 . . . . . . 1 . . . . . . . . . . . .
## 3  1 . . . . . . . . . . . 1 . . . . . . .
## 4  1 . . . . . . . . . . . . . . . . 1 . .
## 5  1 . . . . . . . . . . . . . . . 1 . . .
## 6  1 . . . 1 . . . . . . . . . . . . . . .
## 7  1 . . . . . . . . . . . . . . 1 . . . .
## 8  1 . . . . . . . . . . . 1 . . . . . . .
## 9  1 . . . . . . . . . . . . . . 1 . . . .
## 10 1 . . . . . . . . . . . 1 . . . . . . .
{% endhighlight %}

Many operations, such as subsetting and multiplication, translate
directly to sparse matrices. We'll construct training and validation
sets as before:


{% highlight r %}
X_train <- X[flights$train_id == "train",]
X_valid <- X[flights$train_id == "valid",]
{% endhighlight %}

The **MatrixModels** package has experimental support for computing
linear regression models with sparse matricies using the function
`lm.fit.sparse`. Because it is experimental, we need to prepend the
notation `MatrixModels:::` to the function call:


{% highlight r %}
beta <- MatrixModels:::lm.fit.sparse(X_train, y_train)
beta
{% endhighlight %}



{% highlight text %}
##  [1]  1.45336226  0.45833410 -0.99573514  0.19141386  0.30913774
##  [6]  0.27300590 -0.19697928  0.91240956 -0.68686479 -0.02243442
## [11] -0.94686875  0.45032046 -0.20260083  0.30558313 -0.13428070
## [16]  0.19623614  0.06097466 -0.39594176  0.52541758  0.23031121
{% endhighlight %}

Notice that this yields the same regression vector beta as the dense
operations above.

## Lumping categorical variables

Some categorical variables come from fixed categories and/or have
relatively equal number of observations in each unique value.
Common examples are day of the week, month of the year, and
weather ("clear", "cloudy", "rainy", "snowing"). Other times we
have cases where a categorical variable has some categories with
many observations but others with very few. In the latter case,
we may not be able to reliably fit a seperate model term to the
rare unique values in the category. The package **forcats**
offers a solution with the function **fct_lump**. This function
lumps together all but the top n categories into an "other"
label. For example, here we can lump all but the top four 
carriers into an other group:


{% highlight r %}
library(forcats)
model <- lm(delayed ~ fct_lump(flights$carrier, n = 4), data = flights)
model
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = delayed ~ fct_lump(flights$carrier, n = 4), data = flights)
## 
## Coefficients:
##                           (Intercept)  
##                                1.9002  
##    fct_lump(flights$carrier, n = 4)OO  
##                               -0.6264  
##    fct_lump(flights$carrier, n = 4)US  
##                               -0.4480  
##    fct_lump(flights$carrier, n = 4)WN  
##                               -0.8362  
## fct_lump(flights$carrier, n = 4)Other  
##                               -0.2838
{% endhighlight %}

Remember that the design matrix *hides* one level in the baseline. Here
the baseline (AA) stands for American Airlines, revealing why all
the coefficents are negative!



