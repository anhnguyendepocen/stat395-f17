---
title: "Class 04: Fuel Efficiency in the Big City"
author: "Taylor Arnold"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "../assets/2017-09-07-class04/")
knitr::opts_chunk$set(fig.height = 4)
knitr::opts_chunk$set(fig.width = 6.8)
```

```{r, message = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
```


### Aesthetics

The `qplot` function comes from the **ggplot2** package. In
**ggplot2**, plots can be modified by changing what are known
as aesthetic mappings. Common aesthetics are: color, size, and
alpha (the opacity of the points). To change any of these to
a fixed value for all points, simply use the `I` function
in the call to `qplot`:

```{r}
qplot(vore, awake, data = msleep, color = I("gold"),
      size = I(10), alpha = I(0.3))
```

Alternatively, we can map aesthetics to variables in our dataset.
For example, let's change the color to a function of the diet
type of each mammal (note that here we do not use the `I` function):

```{r}
qplot(bodywt_log, awake, data = msleep, color = vore)
```

Color behaves differently if we use a discrete variable or a numeric
one. Notice here how a color gradient is used in place of a fixed scale:

```{r}
qplot(bodywt_log, awake, data = msleep, color = bodywt_log)
```

This last plot is particularly hard to read on the projector. It is also
difficult on my screen and for anyone with any degree of color-blindness.
The package **viridis** can help. Let's load it in (if you want to run this
line, make sure to install the package first!):

```{r}
library(viridis)
```

We now literally add a *layer* to the plot describing what color scale we
would like to make use of:

```{r}
qplot(bodywt_log, awake, data = msleep, color = bodywt_log) +
  scale_color_viridis()
```

We will make use of these aesthetics throughout the semester to explore datasets.
In particular, the color mapping allows us to look at more than two variables at
the same time.


## Evaluating models

How should be evaluate the predictive ability of a model?
The method of oridinary least squares suggests that we
should consider the sum of squared errors. A challenge with
this is that the value of a sum will grow in proportion to
how many observations there are. This makes it hard to compare
results across data sources and subsets. A simple solution
is to use the average value of the squared errors, known as
the mean squared error or MSE:

```{r, warning=FALSE}
mean((tea$score - tea$score_pred)^2, na.rm = TRUE)
```

This works as a general measurment of error, but the units
are a bit strange as they are given in squared scores. A
simple solution to this exists by taking the square root
of the MSE, resulting in the root mean squared error (RMSE):

```{r, warning=FALSE}
sqrt(mean((tea$score - tea$score_pred)^2, na.rm = TRUE))
```

You'll find references to the RMSE through the literature on
machine learning as it is by far the most common measurment
for predictiveness of continuous responses.

How good of a result is our RMSE of 1.99? It's hard to say
for sure, but one easy thing to do is to compare it to the
simpliest possible model: the one that predicts every score
will be equal to the average score. The RMSE of this estimator
is given by:

```{r, warning=FALSE}
sqrt(mean((tea$score - mean(tea$score, na.rm = TRUE))^2, na.rm = TRUE))
```

So, we have improved on the baseline model, but not by a large
amount. Note: if you are familiar with the standard error, the
RMSE of the mean is *almost* equal to the standard error of
the response.

## Validation Sets

One potential difficulty with the approach above is that we are
using the same data to validate models that we used to construct
them. In this simple case it is unlikely to cause problems, but
it will be a huge problem going forward as we look towards more
complex prediction schemes.

So far, I have discuss that the observations are split into two
groups: those where we know the response and those where we need
to predict the responses. Looking at a table of the `train_id`
variable we see that their are actual three subsets of data:

```{r}
table(tea$train_id)
```

The response is missing only on the 48 observations labels with
the test flag. What is the purpose of the validation label? When
building models, we should only train using the training set.
The validation set then can be used to *validate* how good the
model is working. We can use it to make decisions about which of
the various models we want to use for our final prediction.

In order to only using the training set when fitting a model with
the `lm` function, we add an option called `subset`:

```{r}
model <- lm(score ~ num_reviews, data = tea,
            subset = train_id == "train")
tea$score_pred <- predict(model, newdata = tea)
model
```

Notice that the exact values for the linear model have changed slighty,
though as mentioned previously in this simple case we would not expect
the parameters to change much when removing the validation set.

Now, we will update our RMSE code to produce the RMSE on each training
id type; the testing variable will be missing (`NA`) for you, but when
I run this code on the full data I will be able to see how well you
did on the hidden data. Don't worry about the specifics of the function
`tapply` here. I suggest just copying this code whenever you need it
and changing the response and dataset names as appropriate:

```{r}
sqrt(tapply((tea$score - tea$score_pred)^2, tea$train_id, mean))
```

Notice here that the training dataset actually has a worse RMSE than
the validation data. This is a common phenomenon with small and medium
sized datasets but often strikes students as very confusing. Shouldn't
the model do better on data it was trained with compared to data it
was not trained on?

The explanation is simply that, due to random chance, the validation
data is noisier than the training data. So, naturally the RMSE of a
simple model is lower on the validation set regardless of which
chunk the model was fit on.


## Multivariate regression with categorical data

We can just as easily add categorical data into our
model. Next week we will cover the specifics of what
is internally being done here, but for now let's just
see what adding the `type` variable to the model does
to the output:

```{r}
model <- lm(score ~ num_reviews + type + price,
            data = tea,
            subset = train_id == "train")
tea$score_pred <- predict(model, newdata = tea)
model
```

Notice that it appears that we now have a seperate term for
each type of tea. If you look more carefully you'll see that
there is no mention of black tea in the list. We will see why
this is next week as well.

The RMSE of this new model makes improvements on both the
training and validation sets:

```{r}
sqrt(tapply((tea$score - tea$score_pred)^2, tea$train_id, mean))
```

However, notice that the training set improved significantly
more than the validation set. We are starting to see the effects
of what is known as *overfitting*, where the model starts
predicting random noises in the training data that do not generalize
into the validation set. This will be a major theme throughout
the entire semester.


