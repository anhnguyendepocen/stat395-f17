---
title: "Class 04: Fuel Efficiency in the Big City"
author: "Taylor Arnold"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "../assets/2017-09-07-class04/")
knitr::opts_chunk$set(fig.height = 4)
knitr::opts_chunk$set(fig.width = 6.8)
```

```{r, message = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
```

## Validation Sets

One potential difficulty with the approach above is that we are
using the same data to validate models that we used to construct
them. In this simple case it is unlikely to cause problems, but
it will be a huge problem going forward as we look towards more
complex prediction schemes.

So far, I have discuss that the observations are split into two
groups: those where we know the response and those where we need
to predict the responses. Looking at a table of the `train_id`
variable we see that their are actual three subsets of data:

```{r}
table(tea$train_id)
```

The response is missing only on the 48 observations labels with
the test flag. What is the purpose of the validation label? When
building models, we should only train using the training set.
The validation set then can be used to *validate* how good the
model is working. We can use it to make decisions about which of
the various models we want to use for our final prediction.

In order to only using the training set when fitting a model with
the `lm` function, we add an option called `subset`:

```{r}
model <- lm(score ~ num_reviews, data = tea,
            subset = train_id == "train")
tea$score_pred <- predict(model, newdata = tea)
model
```

Notice that the exact values for the linear model have changed slighty,
though as mentioned previously in this simple case we would not expect
the parameters to change much when removing the validation set.

Now, we will update our RMSE code to produce the RMSE on each training
id type; the testing variable will be missing (`NA`) for you, but when
I run this code on the full data I will be able to see how well you
did on the hidden data. Don't worry about the specifics of the function
`tapply` here. I suggest just copying this code whenever you need it
and changing the response and dataset names as appropriate:

```{r}
sqrt(tapply((tea$score - tea$score_pred)^2, tea$train_id, mean))
```

Notice here that the training dataset actually has a worse RMSE than
the validation data. This is a common phenomenon with small and medium
sized datasets but often strikes students as very confusing. Shouldn't
the model do better on data it was trained with compared to data it
was not trained on?

The explanation is simply that, due to random chance, the validation
data is noisier than the training data. So, naturally the RMSE of a
simple model is lower on the validation set regardless of which
chunk the model was fit on.


## Multivariate regression with categorical data

We can just as easily add categorical data into our
model. Next week we will cover the specifics of what
is internally being done here, but for now let's just
see what adding the `type` variable to the model does
to the output:

```{r}
model <- lm(score ~ num_reviews + type + price,
            data = tea,
            subset = train_id == "train")
tea$score_pred <- predict(model, newdata = tea)
model
```

Notice that it appears that we now have a seperate term for
each type of tea. If you look more carefully you'll see that
there is no mention of black tea in the list. We will see why
this is next week as well.

The RMSE of this new model makes improvements on both the
training and validation sets:

```{r}
sqrt(tapply((tea$score - tea$score_pred)^2, tea$train_id, mean))
```

However, notice that the training set improved significantly
more than the validation set. We are starting to see the effects
of what is known as *overfitting*, where the model starts
predicting random noises in the training data that do not generalize
into the validation set. This will be a major theme throughout
the entire semester.


