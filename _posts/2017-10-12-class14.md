---
title: "Class 14: All Hail the Chief"
author: "Taylor Arnold"
output: html_notebook
---




{% highlight r %}
library(readr)
library(dplyr)
library(ggplot2)
library(tokenizers)
library(stringi)
library(smodels)
{% endhighlight %}


{% highlight r %}
president <- read_csv("~/files/ml_data/presidents3.csv")
{% endhighlight %}


{% highlight r %}
stri_wrap(president$text[1000], width = 60)
{% endhighlight %}



{% highlight text %}
##  [1] "Iraqis are showing their courage every day, and we are proud"
##  [2] "to be their allies in the cause of freedom. Our work in Iraq"
##  [3] "is difficult because our enemy is brutal. But that brutality"
##  [4] "has not stopped the dramatic progress of a new democracy."
##  [5] "In less than 3 years, the nation has gone from dictatorship"
##  [6] "to liberation, to sovereignty, to a Constitution, to"
##  [7] "national elections. At the same time, our coalition has been"
##  [8] "relentless in shutting off terrorist infiltration, clearing"
##  [9] "out insurgent strongholds, and turning over territory to"
## [10] "Iraqi security forces."
{% endhighlight %}


{% highlight r %}
data(stop_words, package = "tidytext")
stop_words <- stop_words$word
sample(stop_words, 50)
{% endhighlight %}



{% highlight text %}
##  [1] "not"         "high"        "older"       "that"        "anyway"
##  [6] "other"       "hereafter"   "somewhere"   "why"         "that's"
## [11] "they'll"     "without"     "shan't"      "rd"          "how"
## [16] "put"         "come"        "several"     "next"        "instead"
## [21] "didn't"      "thoroughly"  "going"       "kept"        "could"
## [26] "hadn't"      "quite"       "most"        "from"        "today"
## [31] "differently" "where"       "anyhow"      "under"       "what"
## [36] "where"       "when"        "either"      "of"          "rather"
## [41] "used"        "certainly"   "fact"        "into"        "work"
## [46] "need"        "new"         "all"         "themselves"  "group"
{% endhighlight %}



{% highlight r %}
token_list <- tokenize_words(president$text,
                             stopwords = stop_words)
token_df <- term_list_to_df(token_list)

X <- term_df_to_matrix(token_df, min_df = 0.03, max_df = 0.97,
                       scale = TRUE)

y <- president$class
X_train <- X[president$train_id == "train",]
X_valid <- X[president$train_id == "valid",]
y_train <- y[president$train_id == "train"]
y_valid <- y[president$train_id == "valid"]

library(glmnet)
model <- cv.glmnet(X_train, y_train, family = "multinomial")
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -86); Convergence for 86th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -65); Convergence for 65th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -75); Convergence for 75th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -73); Convergence for 73th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -74); Convergence for 74th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -72); Convergence for 72th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -72); Convergence for 72th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -89); Convergence for 89th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -67); Convergence for 67th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -76); Convergence for 76th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight r %}
plot(model)
{% endhighlight %}

![plot of chunk unnamed-chunk-5](../assets/2017-10-12-class14/unnamed-chunk-5-1.png)


{% highlight r %}
beta <- coef(model, s = model$lambda[12])
length(beta)
{% endhighlight %}



{% highlight text %}
## [1] 3
{% endhighlight %}



{% highlight r %}
head(beta[[3]])
{% endhighlight %}



{% highlight text %}
## 6 x 1 sparse Matrix of class "dgCMatrix"
##                      1
## (Intercept) 0.03479369
## people      0.46827615
## america     .
## american    .
## americans   .
## country     .
{% endhighlight %}


{% highlight r %}
for(j in 1:3) {
  beta <- coef(model, s = model$lambda[12])
  cat(c( "Obama", "Bush", "Clinton" )[j])
  cat(":\n")
  out <- paste(rownames(beta[[j]])[which((beta[[j]] != 0))], " (",
        sign(beta[[j]])[which((beta[[j]] != 0))], ")", sep = "")
  print(out)
  cat("\n")
}
{% endhighlight %}



{% highlight text %}
## Obama:
##  [1] "(Intercept) (-1)" "jobs (1)"         "future (1)"
##  [4] "businesses (1)"   "job (1)"          "peace (-1)"
##  [7] "hope (-1)"        "income (-1)"      "billion (-1)"
## [10] "lead (1)"         "increase (-1)"    "research (1)"
## [13] "invest (1)"       "afford (1)"       "finally (1)"
##
## Bush:
##  [1] "(Intercept) (1)"  "jobs (-1)"        "security (1)"
##  [4] "deficit (-1)"     "freedom (1)"      "weapons (1)"
##  [7] "college (-1)"     "social (1)"       "free (1)"
## [10] "hope (1)"         "iraq (1)"         "terrorists (1)"
## [13] "communities (-1)" "income (1)"       "billion (1)"
## [16] "terror (1)"       "republicans (-1)"
##
## Clinton:
##  [1] "(Intercept) (1)" "people (1)"      "energy (-1)"
##  [4] "cut (1)"         "national (1)"    "welfare (1)"
##  [7] "bill (1)"        "free (-1)"       "century (1)"
## [10] "program (1)"     "crime (1)"       "service (1)"
{% endhighlight %}


{% highlight r %}
y_valid_pred <- predict(model, X_valid, type = "response",
                        s = model$lambda.min)[,,1]
head(y_valid_pred)
{% endhighlight %}



{% highlight text %}
##                1          2         3
## [1,] 0.119140063 0.56832098 0.3125390
## [2,] 0.075003795 0.61724273 0.3077535
## [3,] 0.371000864 0.09669129 0.5323078
## [4,] 0.004256906 0.01190841 0.9838347
## [5,] 0.302853653 0.18828918 0.5088572
## [6,] 0.183596798 0.27580156 0.5406016
{% endhighlight %}



{% highlight r %}
y_valid_pred <- predict(model, X_valid, type = "response",
                        s = model$lambda.min)[,,1]
y_valid_pred <- apply(y_valid_pred, 1, which.max)
mean(y_valid == y_valid_pred)
{% endhighlight %}



{% highlight text %}
## [1] 0.5732484
{% endhighlight %}

## Bigrams


{% highlight r %}
token_list <- tokenize_ngrams(president$text, n = 2, n_min = 1,
                             stopwords = stop_words)
token_df <- term_list_to_df(token_list)

X <- term_df_to_matrix(token_df, min_df = 0.03)
y <- president$class
X_train <- X[president$train_id == "train",]
X_valid <- X[president$train_id == "valid",]
y_train <- y[president$train_id == "train"]
y_valid <- y[president$train_id == "valid"]

library(glmnet)
model <- cv.glmnet(X_train, y_train, family = "multinomial")
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -60); Convergence for 60th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -84); Convergence for 84th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -76); Convergence for 76th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -67); Convergence for 67th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -80); Convergence for 80th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -71); Convergence for 71th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -79); Convergence for 79th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -65); Convergence for 65th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -68); Convergence for 68th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -70); Convergence for 70th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight r %}
plot(model)
{% endhighlight %}

![plot of chunk unnamed-chunk-10](../assets/2017-10-12-class14/unnamed-chunk-10-1.png)


{% highlight r %}
for(j in 1:3) {
  beta <- coef(model, s = model$lambda[15])
  cat(c( "Obama", "Bush", "Clinton" )[j])
  cat(":\n")
  out <- paste(rownames(beta[[j]])[which((beta[[j]] != 0))], " (",
        sign(beta[[j]])[which((beta[[j]] != 0))], ")", sep = "")
  print(out)
  cat("\n")
}
{% endhighlight %}



{% highlight text %}
## Obama:
##  [1] "(Intercept) (1)"    "country (-1)"       "jobs (1)"
##  [4] "children (-1)"      "budget (-1)"        "education (1)"
##  [7] "citizens (-1)"      "businesses (1)"     "job (1)"
## [10] "deficit (1)"        "president (-1)"     "peace (-1)"
## [13] "pass (-1)"          "hope (-1)"          "taxes (-1)"
## [16] "strong (-1)"        "fellow (-1)"        "income (-1)"
## [19] "billion (-1)"       "lead (1)"           "increase (-1)"
## [22] "research (1)"       "administration (1)" "invest (1)"
## [25] "afford (1)"         "progress (1)"       "expand (-1)"
## [28] "finally (1)"        "republicans (1)"
##
## Bush:
##  [1] "(Intercept) (1)"     "american (-1)"       "jobs (-1)"
##  [4] "families (-1)"       "education (-1)"      "future (-1)"
##  [7] "citizens (1)"        "home (-1)"           "deficit (-1)"
## [10] "money (1)"           "federal (1)"         "freedom (1)"
## [13] "change (-1)"         "hard (-1)"           "weapons (1)"
## [16] "continue (1)"        "college (-1)"        "life (1)"
## [19] "free (1)"            "hope (1)"            "iraq (1)"
## [22] "terrorists (1)"      "social security (1)" "cuts (-1)"
## [25] "communities (-1)"    "military (1)"        "fight (1)"
## [28] "income (1)"          "billion (1)"         "class (-1)"
## [31] "terror (1)"          "coverage (1)"        "stand (-1)"
## [34] "bipartisan (-1)"     "global (-1)"         "raise (-1)"
## [37] "finally (-1)"        "republicans (-1)"
##
## Clinton:
##  [1] "(Intercept) (-1)" "people (1)"       "energy (-1)"
##  [4] "united (-1)"      "cut (1)"          "president (1)"
##  [7] "national (1)"     "change (1)"       "challenge (1)"
## [10] "welfare (1)"      "law (-1)"         "pay (1)"
## [13] "bill (1)"         "free (-1)"        "women (-1)"
## [16] "parents (1)"      "protect (-1)"     "goal (-1)"
## [19] "start (1)"        "trade (1)"        "century (1)"
## [22] "program (1)"      "class (1)"        "democracy (1)"
## [25] "crime (1)"        "stop (1)"         "research (-1)"
## [28] "global (1)"       "proud (1)"        "expand (1)"
## [31] "raise (1)"
{% endhighlight %}


{% highlight r %}
y_valid_pred <- predict(model, X_valid, type = "response",
                        s = model$lambda.min)[,,1]
y_valid_pred <- apply(y_valid_pred, 1, which.max)
mean(y_valid == y_valid_pred)
{% endhighlight %}



{% highlight text %}
## [1] 0.611465
{% endhighlight %}

## Character grams


{% highlight r %}
token_list <- tokenize_character_shingles(president$text, n = 3,
                                          n_min = 1,
                                          strip_non_alphanum = TRUE)
token_df <- term_list_to_df(token_list)

X <- term_df_to_matrix(token_df, min_df = 0.03)
y <- president$class
X_train <- X[president$train_id == "train",]
X_valid <- X[president$train_id == "valid",]
y_train <- y[president$train_id == "train"]
y_valid <- y[president$train_id == "valid"]

library(glmnet)
model <- cv.glmnet(X_train, y_train, family = "multinomial")
plot(model)
{% endhighlight %}

![plot of chunk unnamed-chunk-13](../assets/2017-10-12-class14/unnamed-chunk-13-1.png)


{% highlight r %}
for(j in 1:3) {
  beta <- coef(model, s = model$lambda[12])
  cat(c( "Obama", "Bush", "Clinton" )[j])
  cat(":\n")
  out <- paste(rownames(beta[[j]])[which((beta[[j]] != 0))], " (",
        sign(beta[[j]])[which((beta[[j]] != 0))], ")", sep = "")
  print(out)
  cat("\n")
}
{% endhighlight %}



{% highlight text %}
## Obama:
## [1] "(Intercept) (-1)" "tha (1)"          "hat (1)"
## [4] "mu (-1)"          "ats (1)"          "emu (-1)"
##
## Bush:
##  [1] "(Intercept) (1)" "hat (-1)"        "bu (-1)"
##  [4] "ob (-1)"         "now (-1)"        "but (-1)"
##  [7] "las (-1)"        "err (1)"         "ira (1)"
## [10] "add (1)"         "oci (1)"
##
## Clinton:
## [1] "(Intercept) (-1)" "to (1)"           "all (1)"
## [4] "ple (1)"          "tto (1)"          "htt (1)"
## [7] "lfa (1)"          "ink (1)"          "ico (1)"
{% endhighlight %}


{% highlight r %}
y_valid_pred <- predict(model, X_valid, type = "response",
                        s = model$lambda.min)[,,1]
y_valid_pred <- apply(y_valid_pred, 1, which.max)
mean(y_valid == y_valid_pred)
{% endhighlight %}



{% highlight text %}
## [1] 0.6910828
{% endhighlight %}

## Negative examples

Let's redo the simple unigram term analysis:


{% highlight r %}
token_list <- tokenize_words(president$text,
                             stopwords = stop_words)
token_df <- term_list_to_df(token_list)

X <- term_df_to_matrix(token_df, min_df = 0.03, max_df = 0.97,
                       scale = FALSE)

y <- president$class
X_train <- X[president$train_id == "train",]
X_valid <- X[president$train_id == "valid",]
y_train <- y[president$train_id == "train"]
y_valid <- y[president$train_id == "valid"]

library(glmnet)
model <- cv.glmnet(X_train, y_train, family = "multinomial")
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -85); Convergence for 85th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -71); Convergence for 71th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -71); Convergence for 71th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -64); Convergence for 64th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -74); Convergence for 74th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -79); Convergence for 79th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -71); Convergence for 71th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -71); Convergence for 71th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}



{% highlight text %}
## Warning: from glmnet Fortran code (error code -72); Convergence for 72th
## lambda value not reached after maxit=100000 iterations; solutions for
## larger lambdas returned
{% endhighlight %}


{% highlight r %}
pnames <-c( "Obama", "Bush", "Clinton" )
table(actual = pnames[y_valid],
      pred = pnames[y_valid_pred])
{% endhighlight %}



{% highlight text %}
##          pred
## actual    Bush Clinton Obama
##   Bush      70      12    16
##   Clinton   12      71    25
##   Obama     10      22    76
{% endhighlight %}



{% highlight r %}
neg_example <- function(actual, pred) {
  pnames <-c( "Obama", "Bush", "Clinton" )
  ids <- which(y_valid == match(actual, pnames) &
               y_valid_pred == match(pred, pnames))
  cat(c(stri_wrap(president$text[sample(ids, 1)], width = 60),
    "\n"), sep = "\n")
}


neg_example("Bush", "Clinton")
{% endhighlight %}



{% highlight text %}
## For too long our culture has said, "If it feels good, do
## it." Now America is embracing a new ethic and a new creed,
## "Let's roll." In the sacrifice of soldiers, the fierce
## brotherhood of firefighters, and the bravery and generosity
## of ordinary citizens, we have glimpsed what a new culture of
## responsibility could look like. We want to be a nation that
## serves goals larger than self. We've been offered a unique
## opportunity, and we must not let this moment pass.
{% endhighlight %}



{% highlight r %}
neg_example("Bush", "Obama")
{% endhighlight %}



{% highlight text %}
## An autoworker fine-tuned some of the best, most fuel-
## efficient cars in the world and did his part to help America
## wean itself off foreign oil. A farmer prepared for the
## spring after the strongest 5-year stretch of farm exports
## in our history. A rural doctor gave a young child the first
## prescription to treat asthma that his mother could afford. A
## man took the bus home from the graveyard shift, bone-tired,
## but dreaming big dreams for his son. And in tight-knit
## communities all across America, fathers and mothers will
## tuck in their kids, put an arm around their spouse, remember
## fallen comrades, and give thanks for being home from a war
## that after 12 long years is finally coming to an end.
{% endhighlight %}



{% highlight r %}
neg_example("Obama", "Bush")
{% endhighlight %}



{% highlight text %}
## My budget does not attempt to solve every problem or address
## every issue. It reflects the stark reality of what we've
## inherited, a trillion-dollar deficit, a financial crisis,
## and a costly recession. Given these realities, everyone
## in this Chamber, Democrats and Republicans, will have to
## sacrifice some worthy priorities for which there are no
## dollars. And that includes me. But that does not mean we can
## afford to ignore our long-term challenges.
{% endhighlight %}

## Visualization


{% highlight r %}
library(irlba)
X_pca <- prcomp_irlba(X_train, n = 2)$x
qplot(X_pca[,1], X_pca[,2], color = factor(y_train),
      alpha = I(0.3)) +
  viridis::scale_color_viridis(discrete = TRUE) +
  theme_minimal()
{% endhighlight %}

![plot of chunk unnamed-chunk-19](../assets/2017-10-12-class14/unnamed-chunk-19-1.png)

## SVD


$$ X = U \cdot D \cdot V^t $$


![](img/tm.jpg)


![](img/svd_fb.png)





{% highlight r %}
library(irlba)
X_svd <- irlba::irlba(X_train, nv = 10)

names(X_svd)
{% endhighlight %}



{% highlight text %}
## [1] "d"     "u"     "v"     "iter"  "mprod"
{% endhighlight %}



{% highlight r %}
length(X_svd$d)
{% endhighlight %}



{% highlight text %}
## [1] 10
{% endhighlight %}



{% highlight r %}
dim(X_svd$u)
{% endhighlight %}



{% highlight text %}
## [1] 651  10
{% endhighlight %}



{% highlight r %}
dim(X_svd$v)
{% endhighlight %}



{% highlight text %}
## [1] 180  10
{% endhighlight %}


{% highlight r %}
top_words <- apply(X_svd$v, 2, function(v) {
  id <- order(v, decreasing = TRUE)[1:5]
  stri_paste(colnames(X_train)[id], collapse = "; ")
})
top_words
{% endhighlight %}



{% highlight text %}
##  [1] "crime; laughter; administration; house; left"
##  [2] "people; america; world; american; government"
##  [3] "health; people; care; world; insurance"
##  [4] "america; world; energy; country; security"
##  [5] "security; social; people; tax; benefits"
##  [6] "government; budget; spending; security; programs"
##  [7] "energy; jobs; people; clean; congress"
##  [8] "security; country; social; children; schools"
##  [9] "jobs; security; america; social; budget"
## [10] "people; energy; america; tax; education"
{% endhighlight %}


{% highlight r %}
table(top_words[apply(X_svd$u, 1, which.max)], pnames[y_train])
{% endhighlight %}



{% highlight text %}
##
##                                                    Bush Clinton Obama
##   america; world; energy; country; security          49      40    30
##   crime; laughter; administration; house; left        2       0     0
##   energy; jobs; people; clean; congress              16      11    33
##   government; budget; spending; security; programs   33      34    30
##   health; people; care; world; insurance             20      12    14
##   jobs; security; america; social; budget            10      26    29
##   people; america; world; american; government       20      15    18
##   people; energy; america; tax; education            14      29    19
##   security; country; social; children; schools       20      47    28
##   security; social; people; tax; benefits            37       7     8
{% endhighlight %}


