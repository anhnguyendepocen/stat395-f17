---
title: "Class 14: Chicago - Now With 80% More Crimes!"
author: "Taylor Arnold"
output: html_notebook
---




{% highlight r %}
library(readr)
library(ggplot2)
library(dplyr)
library(methods)
library(keras)
{% endhighlight %}


{% highlight r %}
crimes <- read_csv("~/files/ml_data/chi_crimes_12.csv")
{% endhighlight %}

$$ Y = \sigma(X \cdot A) \cdot B $$



{% highlight r %}
to_categorical(c(1,1,2,4,10))
{% endhighlight %}



{% highlight text %}
##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
## [1,]    0    1    0    0    0    0    0    0    0     0     0
## [2,]    0    1    0    0    0    0    0    0    0     0     0
## [3,]    0    0    1    0    0    0    0    0    0     0     0
## [4,]    0    0    0    0    1    0    0    0    0     0     0
## [5,]    0    0    0    0    0    0    0    0    0     0     1
{% endhighlight %}


{% highlight r %}
X <- scale(as.matrix(select(crimes, longitude, latitude, hour)))
y <- factor(crimes$crime_type)
crime_levels <- levels(y)
y <- as.integer(y) - 1

X_train <- X[crimes$train_id == "train",]
y_train <- to_categorical(y[crimes$train_id == "train"], num_classes = 12)
X_valid <- X[crimes$train_id == "valid",]
y_valid <- to_categorical(y[crimes$train_id == "valid"], num_classes = 12)
{% endhighlight %}


{% highlight r %}
y_train[sample(nrow(y_train), 10),]
{% endhighlight %}



{% highlight text %}
##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
##  [1,]    0    0    0    0    0    0    0    0    0     1     0     0
##  [2,]    0    0    0    0    0    0    0    1    0     0     0     0
##  [3,]    0    0    0    0    0    0    0    0    0     0     1     0
##  [4,]    0    0    0    0    0    0    0    0    0     0     1     0
##  [5,]    0    0    0    0    0    0    0    0    0     0     0     1
##  [6,]    0    0    0    0    0    0    0    1    0     0     0     0
##  [7,]    1    0    0    0    0    0    0    0    0     0     0     0
##  [8,]    0    0    0    0    0    0    0    0    0     1     0     0
##  [9,]    0    1    0    0    0    0    0    0    0     0     0     0
## [10,]    0    0    0    0    0    0    0    0    0     0     0     1
{% endhighlight %}


{% highlight r %}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 20, input_shape = c(3)) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 20) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 12) %>%
  layer_activation(activation = "softmax")
model
{% endhighlight %}



{% highlight text %}
## Model
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_1 (Dense)                  (None, 20)                    80          
## ___________________________________________________________________________
## activation_1 (Activation)        (None, 20)                    0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 20)                    420         
## ___________________________________________________________________________
## activation_2 (Activation)        (None, 20)                    0           
## ___________________________________________________________________________
## dense_3 (Dense)                  (None, 12)                    252         
## ___________________________________________________________________________
## activation_3 (Activation)        (None, 12)                    0           
## ===========================================================================
## Total params: 752
## Trainable params: 752
## Non-trainable params: 0
## ___________________________________________________________________________
{% endhighlight %}



{% highlight r %}
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
{% endhighlight %}


{% highlight r %}
model %>%
  fit(X_train, y_train, epochs = 10,
      validation_data = list(X_valid, y_valid))
{% endhighlight %}


{% highlight r %}
crimes <- sample_frac(crimes, size = 1)
X <- scale(as.matrix(select(crimes, longitude, latitude, hour)))
y <- factor(crimes$crime_type)
crime_levels <- levels(y)
y <- as.integer(y) - 1

X_train <- X[crimes$train_id == "train",]
y_train <- to_categorical(y[crimes$train_id == "train"], num_classes = 12)
X_valid <- X[crimes$train_id == "valid",]
y_valid <- to_categorical(y[crimes$train_id == "valid"], num_classes = 12)
{% endhighlight %}


{% highlight r %}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 20, input_shape = c(3)) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 20) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 12) %>%
  layer_activation(activation = "softmax")
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
{% endhighlight %}



{% highlight r %}
model %>%
  fit(X_train, y_train, epochs = 10,
      validation_data = list(X_valid, y_valid))
{% endhighlight %}


{% highlight r %}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 20, input_shape = c(3)) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 20) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 12) %>%
  layer_activation(activation = "softmax")
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
{% endhighlight %}



{% highlight r %}
model %>%
  fit(X_train, y_train, epochs = 10,
      validation_data = list(X_valid, y_valid))
{% endhighlight %}

## Backpropagation

$$ \begin{align} \widehat{Y} &= \sigma(X \cdot A) \cdot B \\
                 \widehat{Y} &= Z \cdot B \end{align} \\
                 $$

It will be easier to index of $A$ as a vector, with elements $a_i$ rather than
having to keep track of the 2-dimensional nature of the matrix.

$$ \frac{\partial f}{\partial a_{i}} =
    \sum_k \frac{\partial z_{k}}{\partial a_{i}} \cdot \frac{\partial f}{\partial z_{k}} $$


$$ \frac{\partial f}{\partial z_{k}} =
    \sum_j \frac{\partial \widehat{y}_{j}}{\partial z_{k}} \cdot \frac{\partial f}{\partial \widehat{y}_{j}} $$


![](img/tikz40.png)







