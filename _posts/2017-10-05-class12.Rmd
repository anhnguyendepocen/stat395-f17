---
title: "Class 12: Taxi!"
author: "Taylor Arnold"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "../assets/2017-10-05-class12/")
```

```{r, message = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(methods)
```



```{r, message = FALSE}
taxi <- read_csv("https://statsmaths.github.io/ml_data/nyc_taxi.csv")
```


```{r}
library(forcats)
taxi$pickup_NTACode <- fct_lump(taxi$pickup_NTACode, prop = 0.01)
taxi$dropoff_NTACode <- fct_lump(taxi$dropoff_NTACode, prop = 0.01)
table(taxi$pickup_NTACode)
```

```{r}
X <- model.matrix(~ . -1 ,
                  data = taxi[,-c(1:3)])
y <- taxi$duration

y_train <- y[taxi$train_id == "train"]
y_valid <- y[taxi$train_id == "valid"]
X_train <- X[taxi$train_id == "train",]
X_valid <- X[taxi$train_id == "valid",]
```



## Sparse Matricies

You may have noticed that the design matrix above contains
primarily zeros. With large datasets and even more categories,
we would be wasting a lot of valuable memory holding all of
these values. R has support for objects known as sparse
matricies. These store only the non-zero elements, saving
space when most elements are zero. To access them, we'll load
the **MatrixModels** package and using the function `model.Matrix`
(in general, in R capital letters indicate sparse matrix
variations of dense matrix operations).

```{r}
library(methods)
library(MatrixModels)
X <- model.Matrix(~ carrier , data = flights[,-c(1:3)], sparse = TRUE)
X[1:10,]
```

Many operations, such as subsetting and multiplication, translate
directly to sparse matrices. We'll construct training and validation
sets as before:

```{r}
X_train <- X[flights$train_id == "train",]
X_valid <- X[flights$train_id == "valid",]
```

The **MatrixModels** package has experimental support for computing
linear regression models with sparse matricies using the function
`lm.fit.sparse`. Because it is experimental, we need to prepend the
notation `MatrixModels:::` to the function call:

```{r}
beta <- MatrixModels:::lm.fit.sparse(X_train, y_train)
beta
```

Notice that this yields the same regression vector beta as the dense
operations above.




```{r}
beta <- lm.fit(X_train, y_train)$coef
y_valid_pred <- X_valid %*% beta
sqrt(mean((y_valid - y_valid_pred)^2))
```

```{r}
beta
```

```{r}
library(glmnet)
model <- cv.glmnet(X_train, y_train)
y_valid_pred <- predict(model, newx = X_valid)
sqrt(mean((y_valid - y_valid_pred)^2))
coef(model, s = model$lambda.1se)
```

```{r}
X <- model.matrix(~ poly(trip_distance, hour, degree = 5) + . -1,
                  data = taxi[,-c(1:3)])
y <- taxi$duration

y_train <- y[taxi$train_id == "train"]
y_valid <- y[taxi$train_id == "valid"]
X_train <- X[taxi$train_id == "train",]
X_valid <- X[taxi$train_id == "valid",]
```

```{r}
library(glmnet)
model <- cv.glmnet(X_train, y_train)
y_valid_pred <- predict(model, newx = X_valid)
sqrt(mean((y_valid - y_valid_pred)^2))
coef(model, s = model$lambda.1se)
```

```{r}
model <- lm(trip_distance ~ poly(trip_distance, hour, degree = 5) + .,
            data = taxi[,-c(1:3)])
y <- predict(model, newdata = taxi)
y_valid <- y[taxi$train_id == "valid"]
sqrt(mean((y_valid - y_valid_pred)^2))
```



