---
title: "Class 11: Thinking, Fast and Slow"
author: "Taylor Arnold"
output: html_notebook
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(fig.path = "../assets/2017-10-03-class11/")
```

```{r, message = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(methods)
```

```{r, message = FALSE}
ca <- read_csv("~/files/ml_data/ca_house_price.csv")
```

$$  X \cdot A =
    \left(\begin{array}{cc} X_{1,1} & X_{1,2} \\
                           X_{1,1} & X_{1,2} \\
                           \vdots & \vdots \\
                           X_{n,1} & X_{n, 2} \end{array} \right) \cdot
    \left(\begin{array}{cc} A_{1,1} & A_{1,2} \\
                            A_{2,1} & A_{2, 2} \end{array} \right)
                            =
   \left(\begin{array}{c} X_{1} \\
                           X_{2} \\
                           \vdots \\
                           X_{n} \end{array} \right) \cdot
    \left(\begin{array}{cc} A^{(0)} & A^{(1)}\end{array} \right)
                            =
   \left(\begin{array}{cc} X_{1} \cdot A^{(0)} & X_{1} \cdot A^{(0)} \\
                           X_{2} \cdot A^{(0)} & X_{2} \cdot A^{(0)} \\
                           \vdots & \vdots \\
                           X_{n} \cdot A^{(0)} & X_{n} \cdot A^{(0)} \end{array} \right)
                        $$

$$ \left( X \cdot A \right) \cdot B, \quad B = \left(\begin{array}{c} b_1 \\ b_2 \end{array}\right) $$

```{r, fig.asp=0.33}
par(mfrow = c(1, 3))
plot(0, 0, type = "n", xlim = c(-1, 4), ylim = c(-1, 2),
     axes = FALSE, xlab = "", ylab = "")
abline(0.5, -0.5, lty = "dashed", lwd = 4)
abline(v = 0); abline(h = 0)
box()
plot(0, 0, type = "n", xlim = c(-1, 4), ylim = c(-1, 2),
     axes = FALSE, xlab = "", ylab = "")
mtext("+", side=2, line=2.2, cex=2)
abline(-1, 1, lty = "dashed", lwd = 4)
abline(v = 0); abline(h = 0)
box()
plot(0, 0, type = "n", xlim = c(-1, 4), ylim = c(-1, 2),
     axes = FALSE, xlab = "", ylab = "")
mtext("=", side=2, line=2.2, cex=2, las=1)
abline(-0.5, 0.5, lty = "dashed", col = "orange", lwd = 4)
abline(0.5, -1, lty = "dashed", lwd = 1)
abline(-1, 1, lty = "dashed", lwd = 1)
abline(v = 0); abline(h = 0)
box()
```


```{r, message = FALSE}
library(keras)
```

```{r}
X <- scale(as.matrix(select(ca, latitude)))
y <- scale(ca$median_house_value)

X_train <- X[ca$train_id == "train",]
X_valid <- X[ca$train_id == "valid",]
y_train <- y[ca$train_id == "train"]
y_valid <- y[ca$train_id == "valid"]
```

```{r, message = FALSE}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 2, input_shape = c(1)) %>%
  layer_dense(units = 1)
model
```

```{r}
model %>% compile(
  loss = 'mse',
  optimizer = optimizer_rmsprop(),
  metrics = c('mse')
)
```

```{r}
model %>%
  fit(X_train, y_train, epochs = 5,
      validation_data = list(X_valid, y_valid))
```


![](img/tikz40.png)

```{r}
y_valid_pred <- predict(model, X_valid)
qplot(X_valid, y_valid_pred)
```

$$ \sigma \left( X \cdot A \right) \cdot B, \quad B =
    \left(\begin{array}{c} b_1 \\ b_2 \end{array}\right) $$

**rectified linear unit (relu)**:

$$ \sigma(x) =
\begin{cases}
    x,& \text{if } x\geq 0\\
    0,              & \text{otherwise}
\end{cases}  $$


```{r, fig.asp=0.33}
par(mfrow = c(1, 3))
plot(0, 0, type = "n", xlim = c(-1, 4), ylim = c(-1, 2),
     axes = FALSE, xlab = "", ylab = "")
abline(1, -1, lty = "dashed", lwd = 4)
abline(v = 0); abline(h = 0)
box()
plot(0, 0, type = "n", xlim = c(-1, 4), ylim = c(-1, 2),
     axes = FALSE, xlab = "", ylab = "")
mtext("+", side=2, line=2.2, cex=2)
abline(-3, 1, lty = "dashed", lwd = 4)
abline(v = 0); abline(h = 0)
box()
plot(0, 0, type = "n", xlim = c(-1, 4), ylim = c(-1, 2),
     axes = FALSE, xlab = "", ylab = "")
mtext("=", side=2, line=2.2, cex=2, las=1)
lines(x = c(-3, 1, 3, 5), y = c(4, 0, 0, 2), col = "orange",
      lwd = 6, lty = "dashed")
abline(-3, 1, lty = "dashed", lwd = 1)
abline(1, -1, lty = "dashed", lwd = 1)
abline(v = 0); abline(h = 0)
box()
```



```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 2, input_shape = c(1)) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 1)
model
```

```{r}
model %>% compile(
  loss = 'mse',
  optimizer = optimizer_rmsprop(),
  metrics = c('mse')
)
model %>%
  fit(X_train, y_train, epochs = 5,
      validation_data = list(X_valid, y_valid))
```

```{r}
y_valid_pred <- predict(model, X_valid)
qplot(X_valid, y_valid_pred)
```


```{r}
X <- scale(as.matrix(select(ca, latitude, longitude)))
y <- scale(ca$median_house_value)

X_train <- X[ca$train_id == "train",]
X_valid <- X[ca$train_id == "valid",]
y_train <- y[ca$train_id == "train"]
y_valid <- y[ca$train_id == "valid"]
```

```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 5, input_shape = c(2)) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 3, input_shape = c(2)) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 1)
model
```

```{r}
model %>% compile(
  loss = 'mse',
  optimizer = optimizer_rmsprop(),
  metrics = c('mse')
)
model %>%
  fit(X_train, y_train, epochs = 20,
      validation_data = list(X_valid, y_valid))
```



```{r}
y_valid_pred <- predict(model, X_valid)
qplot(X_valid[,1], X_valid[,2], color = y_valid_pred) +
  viridis::scale_color_viridis()
```


## Stochastic Gradient Descent

$$ \left( w^{(0)} - \eta \cdot \nabla_w f \right) \, \rightarrow \, w^{(1)} $$

$$ \begin{align} f(w) &= \sum_i (\widehat{y}_i(w) - y_i)^2 \\
        &= \sum_i f_i(w) \\
        \nabla_w f &= \sum_i \nabla_w f_i
    \end{align}$$

$$
\begin{align}
\left( w^{(0)} - (\eta / n) \cdot \nabla_{w^{(0)}} f_1 \right) \, &\rightarrow \, w^{(1)} \\
\left( w^{(1)} - (\eta / n) \cdot \nabla_{w^{(0)}} f_2 \right) \, &\rightarrow \, w^{(2)} \\
&\vdots \\
\left( w^{(n-1)} - (\eta / n) \cdot \nabla_{w^{(0)}} f_n \right) \, &\rightarrow \, w^{(n)} \\
\end{align}
$$

The $w^{(n)}$ here is exactly equivalent to the $w^{(1)}$ from before.

$$
\begin{align}
\left( w^{(0)} - \eta' \cdot \nabla_{w^{(0)}} f_1 \right) \, &\rightarrow \, w^{(1)} \\
\left( w^{(1)} - \eta' \cdot \nabla_{w^{(1)}} f_2 \right) \, &\rightarrow \, w^{(2)} \\
&\vdots \\
\left( w^{(n-1)} - \eta' \cdot \nabla_{w^{(n)}} f_n \right) \, &\rightarrow \, w^{(n)} \\
\end{align}
$$




