```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(stringi)
library(tokenizers)
library(smodels)
library(keras)
library(Matrix)
library(methods)
```


```{r}
stylo <- read_csv("https://statsmaths.github.io/ml_data/stylo_uk.csv")
```

```{r}
words <- tokenize_words(stylo$text)
vocab <- top_n(count(data_frame(word = unlist(words)), word), n = 5000)$word
head(vocab, 250)
```

```{r}
id <- lapply(words, function(v) match(v, vocab))
id <- lapply(id, function(v) v[!is.na(v)])
id[1:3]
```

```{r}
X <- pad_sequences(id, maxlen = 100)
X[1:5,]
```


```{r}
y <- stylo$author
X_train <- X[stylo$train_id == "train",]
y_train <- to_categorical(y[stylo$train_id == "train"] - 1, num_classes = 5)
```

```{r}
model <- keras_model_sequential()
model %>%
  layer_embedding(
    input_dim = length(vocab) + 1,
    output_dim = 25,
    input_length = ncol(X)
    ) %>%

  layer_conv_1d(filters = 128, kernel_size = c(4)) %>%
  layer_max_pooling_1d(pool_size = 10L) %>%
  layer_dropout(rate = 0.5) %>%

  layer_flatten() %>%
  layer_dense(256) %>%
  layer_activation("relu")%>%
  layer_dense(ncol(y_train)) %>%
  layer_activation("softmax")

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(lr = 0.003),
  metrics = "accuracy"
)

model
```

```{r}
model %>% fit(X_train, y_train, batch_size = 32,
              epochs = 5,
              validation_split = 0.1)
y_pred <- predict_classes(model, X) + 1
tapply(y == y_pred, stylo$train_id, mean)
```













